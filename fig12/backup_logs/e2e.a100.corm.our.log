model='corm' system='our' seqlen=4096 layer_num=None
input_names=['q', 'k', 'v', 'corm_mask']
output_names=['out', 'corm_score']
graph main_graph (
  %q[FLOAT16, 1x4096x32x128]
  %k[FLOAT16, 1x4096x32x128]
  %v[FLOAT16, 1x4096x32x128]
  %corm_mask[FLOAT, 4096x4096]
) {
  %/Constant_output_0 = Constant[value = <Tensor>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Trilu_output_0 = Trilu[upper = 1](%/Constant_output_0, %/Constant_1_output_0)
  %/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%q)
  %/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%v)
  %/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%k)
  %/MatMul_output_0 = MatMul(%/Transpose_output_0, %/Transpose_2_output_0)
  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_output_0 = Div(%/MatMul_output_0, %/Constant_2_output_0)
  %/Add_output_0 = Add(%/Div_output_0, %/Trilu_output_0)
  %/Cast_output_0 = Cast[to = 1](%/Add_output_0)
  %/Softmax_output_0 = Softmax[axis = -1](%/Cast_output_0)
  %/Cast_1_output_0 = Cast[to = 10](%/Softmax_output_0)
  %/MatMul_1_output_0 = MatMul(%/Cast_1_output_0, %/Transpose_1_output_0)
  %/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/MatMul_1_output_0)
  %/GreaterOrEqual_output_0 = GreaterOrEqual(%/Softmax_output_0, %corm_mask)
  %/Cast_2_output_0 = Cast[to = 7](%/GreaterOrEqual_output_0)
  %/Constant_3_output_0 = Constant[value = <Tensor>]()
  %/ReduceSum_output_0 = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/Cast_2_output_0, %/Constant_3_output_0)
  %/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Greater_output_0 = Greater(%/ReduceSum_output_0, %/Constant_4_output_0)
  %/Constant_5_output_0 = Constant[value = <Tensor>]()
  %out = Reshape[allowzero = 0](%/Transpose_3_output_0, %/Constant_5_output_0)
  %/Constant_6_output_0 = Constant[value = <Tensor>]()
  %corm_score = Reshape[allowzero = 0](%/Greater_output_0, %/Constant_6_output_0)
  return %out, %corm_score
}
/home/ppopp25_ae/ppopp25_ae/3rd/asuka/python/asuka/translate.py:217: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  operand_torch = torch.from_numpy(operand)
module {
  func.func @Corm(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xi8>) {
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.softmax %7, dim = -1 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.convert %8, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.dot %9, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %11 = asuka.permute %10, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %12 = asuka.cmp  GE %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<4096x4096xf32>) -> tensor<1x32x4096x4096xi8>
    %13 = asuka.convert %12, type = i64 : (tensor<1x32x4096x4096xi8>) -> tensor<1x32x4096x4096xi64>
    %14 = asuka.reduce(%13), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xi64>) -> tensor<1x32x4096xi64>
    %cst_0 = arith.constant dense<0> : tensor<1xi64>
    %15 = asuka.cmp  GT %14, %cst_0 : (tensor<1x32x4096xi64>, tensor<1xi64>) -> tensor<1x32x4096xi8>
    %16 = asuka.reshape %11 : (tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16>
    %17 = asuka.reshape %15 : (tensor<1x32x4096xi8>) -> tensor<1x32x4096xi8>
    return %16, %17 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xi8>
  }
}
idx=0 arith.constant
idx=1 asuka.trilu
idx=2 asuka.permute
idx=3 asuka.permute
idx=4 asuka.permute
idx=5 asuka.dot
idx=6 asuka.div
idx=7 asuka.add
idx=8 asuka.convert
idx=9 asuka.exp
idx=10 asuka.reduce
idx=11 asuka.div
idx=12 asuka.convert
idx=13 asuka.dot
idx=14 asuka.permute
idx=15 asuka.cmp
idx=16 asuka.reduce
len(connected_partitions)=1026
pruning bad partition(provide_output=False):   0%|          | 0/1026 [00:00<?, ?it/s]pruning bad partition(provide_output=False):  20%|█▉        | 201/1026 [00:00<00:00, 2004.09it/s]pruning bad partition(provide_output=False):  40%|███▉      | 407/1026 [00:00<00:00, 2035.31it/s]pruning bad partition(provide_output=False):  60%|██████    | 619/1026 [00:00<00:00, 2073.32it/s]pruning bad partition(provide_output=False):  81%|████████  | 828/1026 [00:00<00:00, 2078.09it/s]pruning bad partition(provide_output=False): 100%|██████████| 1026/1026 [00:00<00:00, 2067.05it/s]
after pruning bad partition: len(connected_partitions)=139
after pruning bad para partition: len(connected_partitions)=93
pruning bad para partition:   0%|          | 0/139 [00:00<?, ?it/s]pruning bad para partition:  43%|████▎     | 60/139 [00:00<00:00, 586.10it/s]pruning bad para partition:  86%|████████▌ | 119/139 [00:00<00:00, 554.20it/s]pruning bad para partition: 100%|██████████| 139/139 [00:00<00:00, 561.36it/s]
max_metric:  536870912.0
tall_and_thin tensors: 8
after expand and pruning bad ai partition: len(connected_partitions)=10
expand and pruning bad ai partition:   0%|          | 0/93 [00:00<?, ?it/s]expand and pruning bad ai partition:  38%|███▊      | 35/93 [00:00<00:00, 75.85it/s]expand and pruning bad ai partition:  46%|████▌     | 43/93 [00:00<00:00, 59.46it/s]expand and pruning bad ai partition:  61%|██████▏   | 57/93 [00:01<00:00, 37.47it/s]expand and pruning bad ai partition:  80%|███████▉  | 74/93 [00:01<00:00, 54.05it/s]expand and pruning bad ai partition:  97%|█████████▋| 90/93 [00:01<00:00, 53.53it/s]expand and pruning bad ai partition: 100%|██████████| 93/93 [00:01<00:00, 54.18it/s]
after pruning bad partition(provide_output=True): len(connected_partitions)=5
pruning bad partition(provide_output=True):   0%|          | 0/10 [00:00<?, ?it/s]pruning bad partition(provide_output=True): 100%|██████████| 10/10 [00:00<00:00, 2008.77it/s]
module {
  func.func @Corm(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xi8>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %14 = asuka.cmp  GE %10, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<4096x4096xf32>) -> tensor<1x32x4096x4096xi8>
    %15 = asuka.reduce(%14), dim = 2, op =  ANY, keep_dim = false : (tensor<1x32x4096x4096xi8>) -> tensor<1x32x4096xi8>
    return %13, %15 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xi8>
  }
  asuka.kernel @Corm_p0(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x1xf32>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.div %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.convert %9, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %11 = asuka.dot %10, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %12 = asuka.permute %11, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %12 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Corm_p1(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %9, %13 : tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Corm_p2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x32x4096x1xf32>, %arg3: tensor<4096x4096xf32>) -> tensor<1x32x4096xi8> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.div %7, %arg2 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.cmp  GE %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<4096x4096xf32>) -> tensor<1x32x4096x4096xi8>
    %10 = asuka.reduce(%9), dim = 2, op =  ANY, keep_dim = false : (tensor<1x32x4096x4096xi8>) -> tensor<1x32x4096xi8>
    asuka.return %10 : tensor<1x32x4096xi8>
  }
  asuka.kernel @Corm_p3(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %13 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Corm_p4(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x1xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.reduce(%7), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %8 : tensor<1x32x4096x1xf32>
  }
}
optimize Corm_p0
optimize Corm_p1
optimize Corm_p2
optimize Corm_p3
optimize Corm_p4
tuning time: 3.7841484546661377 sec
Skip op: func.func
path: /tmp/tmpx19ux5_i.py
profiling...
out_str='[Corm_p0] avg_ms: 1.127156138420105\n[Corm_p1] avg_ms: 0.9803896546363831\n[Corm_p2] avg_ms: 1.4153305292129517\n[Corm_p3] avg_ms: 0.9952924251556396\n[Corm_p4] avg_ms: 0.653196394443512\n'
[Corm_p0] avg_ms: 1.127156138420105
[Corm_p1] avg_ms: 0.9803896546363831
[Corm_p2] avg_ms: 1.4153305292129517
[Corm_p3] avg_ms: 0.9952924251556396
[Corm_p4] avg_ms: 0.653196394443512
best_kernel:

Corm_p1
Corm_p2
best_time=2.3957201838493347
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from typing import Callable, Any, Optional, Tuple

def bench_Corm_p1():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  avg_ms = triton.testing.do_bench(lambda: Corm_p1(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[Corm_p1] avg_ms:', avg_ms)

def Corm_p1(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  empty_ptr_4 = torch.empty(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  grid = (1, 32, 32)
  Corm_p1_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_3
  tensor_6 = empty_ptr_4
  return tensor_5, tensor_6

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def Corm_p1_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.275311e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 0
  const_12 = 1
  const_13 = 4096
  const_14 = 128
  mul_15 = pid_6 * const_14
  mul_16 = mul_15 * const_13
  mul_17 = pid_7 * const_14
  add_18 = mul_16 + mul_17
  block_ptr_19 = tl.make_block_ptr(
    base=arg_0 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_20 = tl.load(block_ptr_19)
  mul_21 = pid_7 * const_13
  add_22 = mul_15 + mul_21
  block_ptr_23 = tl.make_block_ptr(
    base=arg_3 + add_22,
    shape=(128, 1,),
    strides=(1, 1,),
    offsets=(0, 0,),
    block_shape=(128, 1,),
    order=(1, 0,),
  )
  block_ptr_24 = tl.make_block_ptr(
    base=arg_4 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  converted_25 = const_8
  mul_26 = block_load_20 * converted_25
  mul_26 = mul_26.to(tl.float16)
  zero_27 = tl.zeros([128, 128], dtype=tl.float32)
  zero_28 = tl.zeros([128, 1], dtype=tl.float32)
  add_29 = mul_15 + const_14
  block_ptr_30 = tl.make_block_ptr(
    base=arg_2 + mul_17,
    shape=(128, 4096,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_31 = tl.make_block_ptr(
    base=arg_1 + mul_17,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_32 in range(const_11, add_29, const_14):
    block_load_33 = tl.load(block_ptr_30)
    block_load_34 = tl.load(block_ptr_31)
    dot_35 = tl.dot(mul_26, block_load_33)
    where_36 = tl.zeros([128, 128], dtype=tl.float32)
    where_36 = tl.where(mul_15 + tl.arange(0, 128)[:, None] >= i_32 + tl.arange(0, 128)[None, :], where_36, float('-inf'))
    add_37 = dot_35 + where_36
    exp2_38 = tl.math.exp2(add_37)
    reduce_sum_39 = tl.sum(exp2_38, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_39 += zero_28
    converted_40 = exp2_38.to(tl.float16)
    dot_41 = tl.dot(converted_40, block_load_34)
    add_42 = zero_27 + dot_41
    block_advance_43 = tl.advance(block_ptr_30, (0, 128,))
    block_advance_44 = tl.advance(block_ptr_31, (128, 0,))
    block_ptr_30 = block_advance_43
    block_ptr_31 = block_advance_44
    zero_27 = add_42
    zero_28 = reduce_sum_39
  div_45 = zero_27 / zero_28
  converted_46 = div_45.to(tl.float16)
  block_store_47 = tl.store(block_ptr_23, zero_28)
  block_store_48 = tl.store(block_ptr_24, converted_46)

def bench_Corm_p2():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  rand_arg_3 = torch.randn(4096, 4096, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: Corm_p2(rand_arg_0, rand_arg_1, rand_arg_2, rand_arg_3))
  print('[Corm_p2] avg_ms:', avg_ms)

def Corm_p2(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  tensor_3 = arg3
  empty_ptr_4 = torch.empty(1, 32, 4096, dtype=torch.bool, device=dev)
  grid = (1, 32, 32)
  Corm_p2_kernel[grid](tensor_0, tensor_1, tensor_2, tensor_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_4
  return tensor_5

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def Corm_p2_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.275311e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 4096
  const_12 = 128
  const_13 = 1
  mul_14 = pid_7 * const_12
  mul_15 = pid_6 * const_12
  mul_16 = mul_15 * const_11
  add_17 = mul_16 + mul_14
  block_ptr_18 = tl.make_block_ptr(
    base=arg_1 + add_17,
    shape=(128, 128,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_load_19 = tl.load(block_ptr_18)
  mul_20 = pid_7 * const_11
  add_21 = mul_15 + mul_20
  block_ptr_22 = tl.make_block_ptr(
    base=arg_4 + add_21,
    shape=(128,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  converted_23 = const_8
  mul_24 = block_load_19 * converted_23
  mul_24 = mul_24.to(tl.float16)
  zero_25 = tl.zeros([128], dtype=tl.int8)
  block_ptr_26 = tl.make_block_ptr(
    base=arg_0 + add_17,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_ptr_27 = tl.make_block_ptr(
    base=arg_2 + add_21,
    shape=(4096,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  add_28 = mul_16 + mul_15
  block_ptr_29 = tl.make_block_ptr(
    base=arg_3 + add_28,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_30 in range(mul_15, const_11, const_12):
    block_load_31 = tl.load(block_ptr_26)
    block_load_32 = tl.load(block_ptr_27)
    block_load_33 = tl.load(block_ptr_29)
    where_34 = tl.zeros([128, 128], dtype=tl.float32)
    where_34 = tl.where(i_30 + tl.arange(0, 128)[:, None] >= mul_15 + tl.arange(0, 128)[None, :], where_34, float('-inf'))
    dot_35 = tl.dot(block_load_31, mul_24)
    add_36 = dot_35 + where_34
    exp2_37 = tl.math.exp2(add_36)
    unsqueeze_38 = block_load_32[:, None]
    div_39 = exp2_37 / unsqueeze_38
    ge_40 = div_39 >= block_load_33
    reduce_any_41 = tl.max(ge_40, axis=0, keep_dims=False).to(tl.int8)
    reduce_any_41 |= zero_25
    block_advance_42 = tl.advance(block_ptr_26, (128, 0,))
    block_advance_43 = tl.advance(block_ptr_27, (128,))
    block_advance_44 = tl.advance(block_ptr_29, (128, 0,))
    block_ptr_26 = block_advance_42
    block_ptr_27 = block_advance_43
    block_ptr_29 = block_advance_44
    zero_25 = reduce_any_41
  block_store_45 = tl.store(block_ptr_22, zero_25)

def Corm(arg_0, arg_1, arg_2, arg_3):
  k0_out_0, k0_out_1 = Corm_p1(arg_0, arg_2, arg_1)
  k1_out_0 = Corm_p2(arg_0, arg_1, k0_out_0, arg_3)
  return k0_out_1, k1_out_0

write code to /tmp/tmppcik0fxr.py
weight_zoo_path='/home/ppopp25_ae/ppopp25_ae/weight_zoo.json'
hf_config.num_hidden_layers=32
data_path='/home/ppopp25_ae/ppopp25_ae/vcsum.jsonl'
token_ids.shape=torch.Size([1, 4096])
token_ids.grad=None
using safe tensor: files={'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors'}
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 126.40it/s]
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:02, 14.25it/s] 12%|█▎        | 4/32 [00:00<00:02, 13.87it/s] 19%|█▉        | 6/32 [00:00<00:01, 13.47it/s] 25%|██▌       | 8/32 [00:00<00:01, 12.57it/s] 31%|███▏      | 10/32 [00:00<00:01, 12.07it/s] 38%|███▊      | 12/32 [00:00<00:01, 11.91it/s] 44%|████▍     | 14/32 [00:01<00:01, 11.89it/s] 50%|█████     | 16/32 [00:01<00:01, 11.20it/s] 56%|█████▋    | 18/32 [00:01<00:01, 10.16it/s] 62%|██████▎   | 20/32 [00:01<00:01,  9.51it/s] 66%|██████▌   | 21/32 [00:01<00:01,  9.27it/s] 69%|██████▉   | 22/32 [00:02<00:01,  8.93it/s] 72%|███████▏  | 23/32 [00:02<00:01,  8.88it/s] 75%|███████▌  | 24/32 [00:02<00:00,  8.45it/s] 78%|███████▊  | 25/32 [00:02<00:00,  7.96it/s] 81%|████████▏ | 26/32 [00:02<00:00,  7.67it/s] 84%|████████▍ | 27/32 [00:02<00:00,  7.43it/s] 88%|████████▊ | 28/32 [00:02<00:00,  7.05it/s] 91%|█████████ | 29/32 [00:03<00:00,  6.85it/s] 94%|█████████▍| 30/32 [00:03<00:00,  6.80it/s] 97%|█████████▋| 31/32 [00:03<00:00,  6.68it/s]100%|██████████| 32/32 [00:03<00:00,  6.87it/s]100%|██████████| 32/32 [00:03<00:00,  9.13it/s]
warmup start
warmup done
[our] avg 332.8021 ms, min 330.5442 ms, max 336.0579 ms (50 runs, 50 warmups, profiled)
