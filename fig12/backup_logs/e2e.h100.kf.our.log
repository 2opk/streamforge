model='kf' system='our' seqlen=4096 layer_num=None
input_names=['q', 'k', 'v', 'exp_rand']
output_names=['out', 'kf_score']
graph main_graph (
  %q[FLOAT16, 1x4096x32x128]
  %k[FLOAT16, 1x4096x32x128]
  %v[FLOAT16, 1x4096x32x128]
  %exp_rand[FLOAT, 1x32x4096x4096]
) {
  %/Constant_output_0 = Constant[value = <Tensor>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Trilu_output_0 = Trilu[upper = 1](%/Constant_output_0, %/Constant_1_output_0)
  %/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%q)
  %/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%v)
  %/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%k)
  %/MatMul_output_0 = MatMul(%/Transpose_output_0, %/Transpose_2_output_0)
  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_output_0 = Div(%/MatMul_output_0, %/Constant_2_output_0)
  %/Add_output_0 = Add(%/Div_output_0, %/Trilu_output_0)
  %/Cast_output_0 = Cast[to = 1](%/Add_output_0)
  %/Softmax_output_0 = Softmax[axis = -1](%/Cast_output_0)
  %/Cast_1_output_0 = Cast[to = 10](%/Softmax_output_0)
  %/MatMul_1_output_0 = MatMul(%/Cast_1_output_0, %/Transpose_1_output_0)
  %/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/MatMul_1_output_0)
  %/Log_output_0 = Log(%exp_rand)
  %/Neg_output_0 = Neg(%/Log_output_0)
  %/Cast_2_output_0 = Cast[to = 1](%/Add_output_0)
  %/Add_1_output_0 = Add(%/Cast_2_output_0, %/Neg_output_0)
  %/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_1_output_0 = Div(%/Add_1_output_0, %/Constant_3_output_0)
  %/Cast_3_output_0 = Cast[to = 1](%/Div_1_output_0)
  %/Softmax_1_output_0 = Softmax[axis = -1](%/Cast_3_output_0)
  %onnx::ReduceSum_30 = Constant[value = <Tensor>]()
  %/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/Softmax_1_output_0, %onnx::ReduceSum_30)
  %/Constant_4_output_0 = Constant[value = <Tensor>]()
  %out = Reshape[allowzero = 0](%/Transpose_3_output_0, %/Constant_4_output_0)
  %/Constant_5_output_0 = Constant[value = <Tensor>]()
  %kf_score = Reshape[allowzero = 0](%/ReduceSum_output_0, %/Constant_5_output_0)
  return %out, %kf_score
}
/home/ppopp25_ae/ppopp25_ae/3rd/asuka/python/asuka/translate.py:217: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  operand_torch = torch.from_numpy(operand)
module {
  func.func @KeyFormer(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>) {
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.softmax %7, dim = -1 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.convert %8, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.dot %9, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %11 = asuka.permute %10, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %12 = asuka.log %arg3 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %13 = asuka.neg %12 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %15 = asuka.add %14, %13 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %cst_0 = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %16 = asuka.div %15, %cst_0 : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %17 = asuka.convert %16, type = f32 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %18 = asuka.softmax %17, dim = -1 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %19 = asuka.reduce(%18), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    %20 = asuka.reshape %11 : (tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16>
    %21 = asuka.reshape %19 : (tensor<1x32x4096xf32>) -> tensor<1x32x4096xf32>
    return %20, %21 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>
  }
}
idx=0 arith.constant
idx=1 arith.constant
idx=2 asuka.trilu
idx=3 asuka.permute
idx=4 asuka.permute
idx=5 asuka.permute
idx=6 asuka.dot
idx=7 asuka.div
idx=8 asuka.add
idx=9 asuka.convert
idx=10 asuka.exp
idx=11 asuka.reduce
idx=12 asuka.div
idx=13 asuka.convert
idx=14 asuka.dot
idx=15 asuka.permute
idx=16 asuka.log
idx=17 asuka.neg
idx=18 asuka.add
idx=19 asuka.div
idx=20 asuka.exp
idx=21 asuka.reduce
idx=22 asuka.div
idx=23 asuka.reduce
len(connected_partitions)=16023
pruning bad partition(provide_output=False):   0%|          | 0/16023 [00:00<?, ?it/s]pruning bad partition(provide_output=False):   1%|          | 153/16023 [00:00<00:10, 1521.95it/s]pruning bad partition(provide_output=False):   2%|▏         | 308/16023 [00:00<00:10, 1534.20it/s]pruning bad partition(provide_output=False):   3%|▎         | 462/16023 [00:00<00:10, 1532.07it/s]pruning bad partition(provide_output=False):   4%|▍         | 620/16023 [00:00<00:09, 1548.12it/s]pruning bad partition(provide_output=False):   5%|▍         | 775/16023 [00:00<00:09, 1541.15it/s]pruning bad partition(provide_output=False):   6%|▌         | 931/16023 [00:00<00:09, 1546.96it/s]pruning bad partition(provide_output=False):   7%|▋         | 1086/16023 [00:00<00:09, 1538.39it/s]pruning bad partition(provide_output=False):   8%|▊         | 1240/16023 [00:00<00:09, 1535.53it/s]pruning bad partition(provide_output=False):   9%|▊         | 1394/16023 [00:00<00:09, 1535.76it/s]pruning bad partition(provide_output=False):  10%|▉         | 1548/16023 [00:01<00:09, 1532.88it/s]pruning bad partition(provide_output=False):  11%|█         | 1709/16023 [00:01<00:09, 1554.38it/s]pruning bad partition(provide_output=False):  12%|█▏        | 1866/16023 [00:01<00:09, 1557.10it/s]pruning bad partition(provide_output=False):  13%|█▎        | 2022/16023 [00:01<00:09, 1545.42it/s]pruning bad partition(provide_output=False):  14%|█▎        | 2177/16023 [00:01<00:09, 1531.23it/s]pruning bad partition(provide_output=False):  15%|█▍        | 2333/16023 [00:01<00:08, 1537.19it/s]pruning bad partition(provide_output=False):  16%|█▌        | 2487/16023 [00:01<00:08, 1534.46it/s]pruning bad partition(provide_output=False):  16%|█▋        | 2641/16023 [00:01<00:08, 1533.89it/s]pruning bad partition(provide_output=False):  17%|█▋        | 2799/16023 [00:01<00:08, 1545.77it/s]pruning bad partition(provide_output=False):  18%|█▊        | 2954/16023 [00:01<00:08, 1542.62it/s]pruning bad partition(provide_output=False):  19%|█▉        | 3109/16023 [00:02<00:08, 1529.20it/s]pruning bad partition(provide_output=False):  20%|██        | 3263/16023 [00:02<00:08, 1531.71it/s]pruning bad partition(provide_output=False):  21%|██▏       | 3417/16023 [00:02<00:08, 1518.26it/s]pruning bad partition(provide_output=False):  22%|██▏       | 3570/16023 [00:02<00:08, 1519.71it/s]pruning bad partition(provide_output=False):  23%|██▎       | 3727/16023 [00:02<00:08, 1531.99it/s]pruning bad partition(provide_output=False):  24%|██▍       | 3884/16023 [00:02<00:07, 1542.60it/s]pruning bad partition(provide_output=False):  25%|██▌       | 4039/16023 [00:02<00:07, 1539.24it/s]pruning bad partition(provide_output=False):  26%|██▌       | 4193/16023 [00:02<00:07, 1532.58it/s]pruning bad partition(provide_output=False):  27%|██▋       | 4347/16023 [00:02<00:07, 1534.37it/s]pruning bad partition(provide_output=False):  28%|██▊       | 4504/16023 [00:02<00:07, 1543.73it/s]pruning bad partition(provide_output=False):  29%|██▉       | 4659/16023 [00:03<00:07, 1534.26it/s]pruning bad partition(provide_output=False):  30%|███       | 4813/16023 [00:03<00:07, 1508.76it/s]pruning bad partition(provide_output=False):  31%|███       | 4964/16023 [00:03<00:07, 1504.30it/s]pruning bad partition(provide_output=False):  32%|███▏      | 5121/16023 [00:03<00:07, 1521.05it/s]pruning bad partition(provide_output=False):  33%|███▎      | 5274/16023 [00:03<00:07, 1518.36it/s]pruning bad partition(provide_output=False):  34%|███▍      | 5426/16023 [00:03<00:07, 1452.43it/s]pruning bad partition(provide_output=False):  35%|███▍      | 5578/16023 [00:03<00:07, 1471.85it/s]pruning bad partition(provide_output=False):  36%|███▌      | 5731/16023 [00:03<00:06, 1488.06it/s]pruning bad partition(provide_output=False):  37%|███▋      | 5886/16023 [00:03<00:06, 1503.38it/s]pruning bad partition(provide_output=False):  38%|███▊      | 6039/16023 [00:03<00:06, 1509.54it/s]pruning bad partition(provide_output=False):  39%|███▊      | 6192/16023 [00:04<00:06, 1515.02it/s]pruning bad partition(provide_output=False):  40%|███▉      | 6344/16023 [00:04<00:06, 1515.89it/s]pruning bad partition(provide_output=False):  41%|████      | 6499/16023 [00:04<00:06, 1524.31it/s]pruning bad partition(provide_output=False):  42%|████▏     | 6654/16023 [00:04<00:06, 1529.13it/s]pruning bad partition(provide_output=False):  43%|████▎     | 6811/16023 [00:04<00:05, 1540.06it/s]pruning bad partition(provide_output=False):  43%|████▎     | 6966/16023 [00:04<00:05, 1539.13it/s]pruning bad partition(provide_output=False):  44%|████▍     | 7120/16023 [00:04<00:05, 1533.93it/s]pruning bad partition(provide_output=False):  45%|████▌     | 7275/16023 [00:04<00:05, 1536.66it/s]pruning bad partition(provide_output=False):  46%|████▋     | 7429/16023 [00:04<00:05, 1536.64it/s]pruning bad partition(provide_output=False):  47%|████▋     | 7583/16023 [00:04<00:05, 1535.95it/s]pruning bad partition(provide_output=False):  48%|████▊     | 7737/16023 [00:05<00:05, 1537.14it/s]pruning bad partition(provide_output=False):  49%|████▉     | 7891/16023 [00:05<00:05, 1537.54it/s]pruning bad partition(provide_output=False):  50%|█████     | 8046/16023 [00:05<00:05, 1540.97it/s]pruning bad partition(provide_output=False):  51%|█████     | 8202/16023 [00:05<00:05, 1545.64it/s]pruning bad partition(provide_output=False):  52%|█████▏    | 8357/16023 [00:05<00:05, 1485.06it/s]pruning bad partition(provide_output=False):  53%|█████▎    | 8514/16023 [00:05<00:04, 1509.62it/s]pruning bad partition(provide_output=False):  54%|█████▍    | 8666/16023 [00:05<00:04, 1511.79it/s]pruning bad partition(provide_output=False):  55%|█████▌    | 8818/16023 [00:05<00:04, 1510.49it/s]pruning bad partition(provide_output=False):  56%|█████▌    | 8974/16023 [00:05<00:04, 1522.27it/s]pruning bad partition(provide_output=False):  57%|█████▋    | 9127/16023 [00:05<00:04, 1508.85it/s]pruning bad partition(provide_output=False):  58%|█████▊    | 9279/16023 [00:06<00:04, 1453.48it/s]pruning bad partition(provide_output=False):  59%|█████▉    | 9432/16023 [00:06<00:04, 1473.30it/s]pruning bad partition(provide_output=False):  60%|█████▉    | 9586/16023 [00:06<00:04, 1491.59it/s]pruning bad partition(provide_output=False):  61%|██████    | 9738/16023 [00:06<00:04, 1498.01it/s]pruning bad partition(provide_output=False):  62%|██████▏   | 9894/16023 [00:06<00:04, 1516.19it/s]pruning bad partition(provide_output=False):  63%|██████▎   | 10046/16023 [00:06<00:03, 1512.50it/s]pruning bad partition(provide_output=False):  64%|██████▎   | 10198/16023 [00:06<00:03, 1512.30it/s]pruning bad partition(provide_output=False):  65%|██████▍   | 10352/16023 [00:06<00:03, 1520.13it/s]pruning bad partition(provide_output=False):  66%|██████▌   | 10505/16023 [00:06<00:03, 1522.23it/s]pruning bad partition(provide_output=False):  67%|██████▋   | 10662/16023 [00:06<00:03, 1534.74it/s]pruning bad partition(provide_output=False):  68%|██████▊   | 10818/16023 [00:07<00:03, 1539.62it/s]pruning bad partition(provide_output=False):  68%|██████▊   | 10972/16023 [00:07<00:03, 1536.63it/s]pruning bad partition(provide_output=False):  69%|██████▉   | 11126/16023 [00:07<00:03, 1536.27it/s]pruning bad partition(provide_output=False):  70%|███████   | 11280/16023 [00:07<00:03, 1528.37it/s]pruning bad partition(provide_output=False):  71%|███████▏  | 11433/16023 [00:07<00:03, 1527.27it/s]pruning bad partition(provide_output=False):  72%|███████▏  | 11588/16023 [00:07<00:02, 1533.39it/s]pruning bad partition(provide_output=False):  73%|███████▎  | 11742/16023 [00:07<00:02, 1473.60it/s]pruning bad partition(provide_output=False):  74%|███████▍  | 11890/16023 [00:07<00:02, 1440.53it/s]pruning bad partition(provide_output=False):  75%|███████▌  | 12040/16023 [00:07<00:02, 1454.90it/s]pruning bad partition(provide_output=False):  76%|███████▌  | 12194/16023 [00:08<00:02, 1479.58it/s]pruning bad partition(provide_output=False):  77%|███████▋  | 12343/16023 [00:08<00:02, 1427.73it/s]pruning bad partition(provide_output=False):  78%|███████▊  | 12497/16023 [00:08<00:02, 1459.30it/s]pruning bad partition(provide_output=False):  79%|███████▉  | 12652/16023 [00:08<00:02, 1484.25it/s]pruning bad partition(provide_output=False):  80%|███████▉  | 12803/16023 [00:08<00:02, 1490.25it/s]pruning bad partition(provide_output=False):  81%|████████  | 12954/16023 [00:08<00:02, 1495.18it/s]pruning bad partition(provide_output=False):  82%|████████▏ | 13108/16023 [00:08<00:01, 1507.58it/s]pruning bad partition(provide_output=False):  83%|████████▎ | 13262/16023 [00:08<00:01, 1516.81it/s]pruning bad partition(provide_output=False):  84%|████████▎ | 13418/16023 [00:08<00:01, 1528.37it/s]pruning bad partition(provide_output=False):  85%|████████▍ | 13577/16023 [00:08<00:01, 1544.99it/s]pruning bad partition(provide_output=False):  86%|████████▌ | 13732/16023 [00:09<00:01, 1541.25it/s]pruning bad partition(provide_output=False):  87%|████████▋ | 13887/16023 [00:09<00:01, 1537.23it/s]pruning bad partition(provide_output=False):  88%|████████▊ | 14041/16023 [00:09<00:01, 1532.00it/s]pruning bad partition(provide_output=False):  89%|████████▊ | 14197/16023 [00:09<00:01, 1537.75it/s]pruning bad partition(provide_output=False):  90%|████████▉ | 14351/16023 [00:09<00:01, 1531.66it/s]pruning bad partition(provide_output=False):  91%|█████████ | 14505/16023 [00:09<00:01, 1514.81it/s]pruning bad partition(provide_output=False):  91%|█████████▏| 14658/16023 [00:09<00:00, 1518.66it/s]pruning bad partition(provide_output=False):  92%|█████████▏| 14812/16023 [00:09<00:00, 1522.62it/s]pruning bad partition(provide_output=False):  93%|█████████▎| 14968/16023 [00:09<00:00, 1532.14it/s]pruning bad partition(provide_output=False):  94%|█████████▍| 15122/16023 [00:09<00:00, 1509.44it/s]pruning bad partition(provide_output=False):  95%|█████████▌| 15274/16023 [00:10<00:00, 1470.43it/s]pruning bad partition(provide_output=False):  96%|█████████▋| 15431/16023 [00:10<00:00, 1495.88it/s]pruning bad partition(provide_output=False):  97%|█████████▋| 15583/16023 [00:10<00:00, 1501.20it/s]pruning bad partition(provide_output=False):  98%|█████████▊| 15734/16023 [00:10<00:00, 1461.70it/s]pruning bad partition(provide_output=False):  99%|█████████▉| 15885/16023 [00:10<00:00, 1474.44it/s]pruning bad partition(provide_output=False): 100%|██████████| 16023/16023 [00:10<00:00, 1515.97it/s]
after pruning bad partition: len(connected_partitions)=974
pruning bad para partition:   0%|          | 0/974 [00:00<?, ?it/s]pruning bad para partition:   4%|▍         | 42/974 [00:00<00:02, 407.71it/s]pruning bad para partition:   9%|▊         | 83/974 [00:00<00:02, 361.13it/s]pruning bad para partition:  12%|█▏        | 120/974 [00:00<00:02, 346.32it/s]pruning bad para partition:  16%|█▌        | 155/974 [00:00<00:02, 345.64it/s]pruning bad para partition:  20%|█▉        | 193/974 [00:00<00:02, 356.94it/s]pruning bad para partition:  24%|██▎       | 229/974 [00:00<00:02, 354.94it/s]pruning bad para partition:  27%|██▋       | 265/974 [00:00<00:02, 354.35it/s]pruning bad para partition:  31%|███       | 301/974 [00:00<00:01, 347.88it/s]pruning bad para partition:  34%|███▍      | 336/974 [00:00<00:01, 343.84it/s]pruning bad para partition:  38%|███▊      | 374/974 [00:01<00:01, 353.53it/s]pruning bad para partition:  42%|████▏     | 410/974 [00:01<00:01, 352.13it/s]pruning bad para partition:  46%|████▌     | 446/974 [00:01<00:01, 350.11it/s]pruning bad para partition:  49%|████▉     | 482/974 [00:01<00:01, 341.88it/s]pruning bad para partition:  53%|█████▎    | 520/974 [00:01<00:01, 351.89it/s]pruning bad para partition:  57%|█████▋    | 556/974 [00:01<00:01, 353.85it/s]pruning bad para partition:  61%|██████    | 594/974 [00:01<00:01, 360.33it/s]pruning bad para partition:  65%|██████▍   | 631/974 [00:01<00:01, 331.87it/s]pruning bad para partition:  68%|██████▊   | 665/974 [00:01<00:00, 319.23it/s]pruning bad para partition:  72%|███████▏  | 703/974 [00:02<00:00, 334.75it/s]pruning bad para partition:  76%|███████▌  | 739/974 [00:02<00:00, 339.88it/s]pruning bad para partition:  79%|███████▉  | 774/974 [00:02<00:00, 340.07it/s]pruning bad para partition:  83%|████████▎ | 809/974 [00:02<00:00, 342.80it/s]pruning bad para partition:  87%|████████▋ | 844/974 [00:02<00:00, 344.83it/s]pruning bad para partition:  90%|█████████ | 879/974 [00:02<00:00, 345.92it/s]pruning bad para partition:  94%|█████████▍| 920/974 [00:02<00:00, 362.95it/s]pruning bad para partition:  98%|█████████▊| 957/974 [00:02<00:00, 355.51it/s]pruning bad para partition: 100%|██████████| 974/974 [00:02<00:00, 348.54it/s]
after pruning bad para partition: len(connected_partitions)=740
max_metric:  536870912.0
tall_and_thin tensors: 10
expand and pruning bad ai partition:   0%|          | 0/740 [00:00<?, ?it/s]expand and pruning bad ai partition:   0%|          | 2/740 [00:01<07:06,  1.73it/s]expand and pruning bad ai partition:   1%|▏         | 10/740 [00:01<01:49,  6.67it/s]expand and pruning bad ai partition:   3%|▎         | 19/740 [00:02<01:01, 11.64it/s]expand and pruning bad ai partition:   3%|▎         | 25/740 [00:02<00:55, 12.92it/s]expand and pruning bad ai partition:   5%|▍         | 35/740 [00:02<00:32, 21.42it/s]expand and pruning bad ai partition:   5%|▌         | 40/740 [00:02<00:30, 22.71it/s]expand and pruning bad ai partition:   6%|▌         | 44/740 [00:02<00:29, 23.50it/s]expand and pruning bad ai partition:   6%|▋         | 48/740 [00:03<00:30, 22.45it/s]expand and pruning bad ai partition:   7%|▋         | 52/740 [00:03<00:32, 21.29it/s]expand and pruning bad ai partition:   8%|▊         | 56/740 [00:03<00:33, 20.69it/s]expand and pruning bad ai partition:   8%|▊         | 59/740 [00:04<00:54, 12.50it/s]expand and pruning bad ai partition:   9%|▉         | 66/740 [00:08<03:48,  2.95it/s]expand and pruning bad ai partition:   9%|▉         | 69/740 [00:09<03:13,  3.47it/s]expand and pruning bad ai partition:  10%|▉         | 73/740 [00:09<02:23,  4.66it/s]expand and pruning bad ai partition:  10%|█         | 76/740 [00:09<02:10,  5.10it/s]expand and pruning bad ai partition:  11%|█▏        | 84/740 [00:10<01:30,  7.24it/s]expand and pruning bad ai partition:  12%|█▏        | 87/740 [00:10<01:36,  6.77it/s]expand and pruning bad ai partition:  13%|█▎        | 93/740 [00:10<01:05,  9.89it/s]expand and pruning bad ai partition:  14%|█▍        | 103/740 [00:11<00:37, 17.01it/s]expand and pruning bad ai partition:  15%|█▍        | 108/740 [00:11<00:35, 17.81it/s]expand and pruning bad ai partition:  15%|█▍        | 108/740 [00:26<00:35, 17.81it/s]expand and pruning bad ai partition:  15%|█▌        | 112/740 [00:40<17:10,  1.64s/it]expand and pruning bad ai partition:  15%|█▌        | 113/740 [00:41<16:43,  1.60s/it]expand and pruning bad ai partition:  16%|█▌        | 118/740 [00:41<10:59,  1.06s/it]expand and pruning bad ai partition:  17%|█▋        | 125/740 [00:41<06:31,  1.57it/s]expand and pruning bad ai partition:  18%|█▊        | 130/740 [00:41<04:40,  2.17it/s]expand and pruning bad ai partition:  18%|█▊        | 134/740 [00:42<03:49,  2.64it/s]expand and pruning bad ai partition:  19%|█▉        | 140/740 [00:42<02:32,  3.94it/s]expand and pruning bad ai partition:  19%|█▉        | 144/740 [00:43<02:36,  3.81it/s]expand and pruning bad ai partition:  20%|█▉        | 147/740 [00:43<02:08,  4.61it/s]expand and pruning bad ai partition:  20%|██        | 150/740 [00:44<01:53,  5.20it/s]expand and pruning bad ai partition:  21%|██▏       | 158/740 [00:44<01:03,  9.14it/s]expand and pruning bad ai partition:  22%|██▏       | 162/740 [00:45<01:30,  6.37it/s]expand and pruning bad ai partition:  22%|██▏       | 165/740 [00:45<01:30,  6.35it/s]expand and pruning bad ai partition:  23%|██▎       | 167/740 [00:55<08:53,  1.07it/s]expand and pruning bad ai partition:  23%|██▎       | 169/740 [00:56<08:10,  1.16it/s]expand and pruning bad ai partition:  24%|██▎       | 174/740 [00:56<05:01,  1.88it/s]expand and pruning bad ai partition:  25%|██▍       | 183/740 [00:56<02:28,  3.75it/s]expand and pruning bad ai partition:  25%|██▌       | 187/740 [00:57<01:56,  4.75it/s]expand and pruning bad ai partition:  26%|██▌       | 190/740 [00:57<01:43,  5.32it/s]expand and pruning bad ai partition:  26%|██▌       | 194/740 [00:57<01:37,  5.58it/s]expand and pruning bad ai partition:  27%|██▋       | 201/740 [00:58<01:16,  7.08it/s]expand and pruning bad ai partition:  27%|██▋       | 203/740 [00:59<01:26,  6.24it/s]expand and pruning bad ai partition:  28%|██▊       | 207/740 [00:59<01:08,  7.82it/s]expand and pruning bad ai partition:  28%|██▊       | 209/740 [00:59<01:02,  8.53it/s]expand and pruning bad ai partition:  29%|██▊       | 212/740 [00:59<00:52, 10.14it/s]expand and pruning bad ai partition:  29%|██▉       | 214/740 [01:00<01:03,  8.24it/s]expand and pruning bad ai partition:  29%|██▉       | 218/740 [01:00<00:47, 11.08it/s]expand and pruning bad ai partition:  30%|███       | 224/740 [01:00<00:30, 16.66it/s]expand and pruning bad ai partition:  31%|███       | 227/740 [01:00<00:34, 14.68it/s]expand and pruning bad ai partition:  31%|███       | 230/740 [01:03<02:32,  3.35it/s]expand and pruning bad ai partition:  31%|███▏      | 232/740 [01:03<02:16,  3.73it/s]expand and pruning bad ai partition:  32%|███▏      | 236/740 [01:03<01:32,  5.44it/s]expand and pruning bad ai partition:  32%|███▏      | 238/740 [01:04<01:42,  4.88it/s]expand and pruning bad ai partition:  33%|███▎      | 244/740 [01:04<01:00,  8.24it/s]expand and pruning bad ai partition:  34%|███▎      | 248/740 [01:06<01:38,  4.99it/s]expand and pruning bad ai partition:  34%|███▍      | 250/740 [01:06<01:32,  5.30it/s]expand and pruning bad ai partition:  34%|███▍      | 252/740 [01:11<05:42,  1.43it/s]expand and pruning bad ai partition:  34%|███▍      | 253/740 [01:12<05:32,  1.46it/s]expand and pruning bad ai partition:  34%|███▍      | 254/740 [01:12<05:21,  1.51it/s]expand and pruning bad ai partition:  34%|███▍      | 255/740 [01:13<04:34,  1.77it/s]expand and pruning bad ai partition:  35%|███▌      | 259/740 [01:13<02:31,  3.17it/s]expand and pruning bad ai partition:  36%|███▋      | 269/740 [01:15<02:03,  3.82it/s]expand and pruning bad ai partition:  37%|███▋      | 271/740 [01:15<01:53,  4.13it/s]expand and pruning bad ai partition:  38%|███▊      | 281/740 [01:16<01:17,  5.90it/s]expand and pruning bad ai partition:  38%|███▊      | 282/740 [01:17<01:15,  6.06it/s]expand and pruning bad ai partition:  39%|███▉      | 290/740 [01:18<01:27,  5.15it/s]expand and pruning bad ai partition:  39%|███▉      | 291/740 [01:19<01:24,  5.32it/s]expand and pruning bad ai partition:  40%|████      | 296/740 [01:19<01:01,  7.20it/s]expand and pruning bad ai partition:  40%|████      | 298/740 [01:19<00:55,  8.00it/s]expand and pruning bad ai partition:  41%|████      | 301/740 [01:19<00:45,  9.57it/s]expand and pruning bad ai partition:  42%|████▏     | 310/740 [01:20<00:44,  9.61it/s]expand and pruning bad ai partition:  42%|████▏     | 312/740 [01:20<00:41, 10.33it/s]expand and pruning bad ai partition:  43%|████▎     | 315/740 [01:20<00:46,  9.19it/s]expand and pruning bad ai partition:  43%|████▎     | 319/740 [01:21<00:40, 10.40it/s]expand and pruning bad ai partition:  44%|████▍     | 326/740 [01:21<00:35, 11.66it/s]expand and pruning bad ai partition:  44%|████▍     | 328/740 [01:21<00:34, 11.78it/s]expand and pruning bad ai partition:  45%|████▍     | 330/740 [01:22<00:36, 11.19it/s]expand and pruning bad ai partition:  45%|████▌     | 335/740 [01:22<00:30, 13.28it/s]expand and pruning bad ai partition:  46%|████▌     | 338/740 [01:22<00:37, 10.60it/s]expand and pruning bad ai partition:  46%|████▌     | 340/740 [01:23<00:40,  9.96it/s]expand and pruning bad ai partition:  46%|████▌     | 342/740 [01:27<03:29,  1.90it/s]expand and pruning bad ai partition:  46%|████▋     | 343/740 [01:27<03:12,  2.06it/s]expand and pruning bad ai partition:  47%|████▋     | 345/740 [01:27<02:24,  2.73it/s]expand and pruning bad ai partition:  47%|████▋     | 348/740 [01:31<04:32,  1.44it/s]expand and pruning bad ai partition:  47%|████▋     | 349/740 [01:31<03:58,  1.64it/s]expand and pruning bad ai partition:  48%|████▊     | 357/740 [01:32<01:41,  3.76it/s]expand and pruning bad ai partition:  49%|████▉     | 361/740 [01:35<03:00,  2.10it/s]expand and pruning bad ai partition:  49%|████▉     | 363/740 [01:37<03:30,  1.79it/s]expand and pruning bad ai partition:  50%|████▉     | 367/740 [01:38<02:45,  2.25it/s]expand and pruning bad ai partition:  50%|████▉     | 368/740 [01:38<02:31,  2.46it/s]expand and pruning bad ai partition:  50%|█████     | 373/740 [01:38<01:28,  4.14it/s]expand and pruning bad ai partition:  51%|█████     | 377/740 [01:38<01:03,  5.73it/s]expand and pruning bad ai partition:  52%|█████▏    | 385/740 [01:39<00:34, 10.34it/s]expand and pruning bad ai partition:  53%|█████▎    | 389/740 [01:39<00:30, 11.48it/s]expand and pruning bad ai partition:  53%|█████▎    | 392/740 [01:39<00:26, 12.92it/s]expand and pruning bad ai partition:  53%|█████▎    | 395/740 [01:39<00:26, 12.96it/s]expand and pruning bad ai partition:  54%|█████▍    | 402/740 [01:39<00:19, 17.47it/s]expand and pruning bad ai partition:  56%|█████▌    | 413/740 [01:40<00:13, 24.69it/s]expand and pruning bad ai partition:  57%|█████▋    | 424/740 [01:40<00:10, 28.92it/s]expand and pruning bad ai partition:  58%|█████▊    | 431/740 [01:40<00:10, 30.03it/s]expand and pruning bad ai partition:  59%|█████▉    | 435/740 [01:40<00:10, 29.46it/s]expand and pruning bad ai partition:  59%|█████▉    | 439/740 [01:41<00:12, 24.38it/s]expand and pruning bad ai partition:  60%|█████▉    | 442/740 [01:41<00:18, 16.53it/s]expand and pruning bad ai partition:  60%|██████    | 445/740 [01:41<00:18, 16.19it/s]expand and pruning bad ai partition:  60%|██████    | 447/740 [01:43<00:58,  4.97it/s]expand and pruning bad ai partition:  61%|██████▏   | 454/740 [01:43<00:34,  8.33it/s]expand and pruning bad ai partition:  62%|██████▏   | 457/740 [01:44<00:35,  7.93it/s]expand and pruning bad ai partition:  62%|██████▏   | 460/740 [01:46<01:14,  3.78it/s]expand and pruning bad ai partition:  63%|██████▎   | 464/740 [01:46<00:53,  5.16it/s]expand and pruning bad ai partition:  64%|██████▍   | 473/740 [01:46<00:29,  8.99it/s]expand and pruning bad ai partition:  64%|██████▍   | 476/740 [01:51<01:37,  2.70it/s]expand and pruning bad ai partition:  65%|██████▍   | 478/740 [01:54<02:38,  1.65it/s]expand and pruning bad ai partition:  65%|██████▌   | 483/740 [01:55<01:44,  2.45it/s]expand and pruning bad ai partition:  66%|██████▌   | 487/740 [01:55<01:15,  3.34it/s]expand and pruning bad ai partition:  66%|██████▋   | 492/740 [01:55<00:53,  4.62it/s]expand and pruning bad ai partition:  67%|██████▋   | 497/740 [01:55<00:42,  5.66it/s]expand and pruning bad ai partition:  68%|██████▊   | 500/740 [01:56<00:37,  6.44it/s]expand and pruning bad ai partition:  68%|██████▊   | 504/740 [01:56<00:28,  8.32it/s]expand and pruning bad ai partition:  69%|██████▉   | 509/740 [01:56<00:22, 10.16it/s]expand and pruning bad ai partition:  71%|███████   | 523/740 [01:56<00:10, 21.11it/s]expand and pruning bad ai partition:  71%|███████▏  | 528/740 [01:57<00:12, 16.64it/s]expand and pruning bad ai partition:  72%|███████▏  | 532/740 [01:57<00:11, 18.19it/s]expand and pruning bad ai partition:  72%|███████▏  | 536/740 [01:58<00:20,  9.95it/s]expand and pruning bad ai partition:  73%|███████▎  | 540/740 [01:59<00:27,  7.35it/s]expand and pruning bad ai partition:  74%|███████▎  | 544/740 [01:59<00:22,  8.58it/s]expand and pruning bad ai partition:  74%|███████▍  | 546/740 [02:02<00:54,  3.53it/s]expand and pruning bad ai partition:  74%|███████▍  | 550/740 [02:02<00:40,  4.68it/s]expand and pruning bad ai partition:  76%|███████▌  | 564/740 [02:02<00:15, 11.13it/s]expand and pruning bad ai partition:  77%|███████▋  | 569/740 [02:02<00:12, 13.48it/s]expand and pruning bad ai partition:  78%|███████▊  | 574/740 [02:10<01:13,  2.26it/s]expand and pruning bad ai partition:  78%|███████▊  | 577/740 [02:10<01:00,  2.70it/s]expand and pruning bad ai partition:  78%|███████▊  | 580/740 [02:12<01:09,  2.29it/s]expand and pruning bad ai partition:  80%|███████▉  | 590/740 [02:13<00:40,  3.71it/s]expand and pruning bad ai partition:  81%|████████  | 596/740 [02:15<00:40,  3.52it/s]expand and pruning bad ai partition:  81%|████████▏ | 602/740 [02:15<00:29,  4.71it/s]expand and pruning bad ai partition:  82%|████████▏ | 604/740 [02:15<00:29,  4.68it/s]expand and pruning bad ai partition:  83%|████████▎ | 614/740 [02:17<00:20,  6.13it/s]expand and pruning bad ai partition:  84%|████████▎ | 619/740 [02:17<00:15,  7.84it/s]expand and pruning bad ai partition:  85%|████████▍ | 627/740 [02:18<00:13,  8.10it/s]expand and pruning bad ai partition:  85%|████████▌ | 630/740 [02:18<00:12,  9.13it/s]expand and pruning bad ai partition:  86%|████████▌ | 637/740 [02:18<00:07, 12.93it/s]expand and pruning bad ai partition:  87%|████████▋ | 641/740 [02:18<00:07, 13.02it/s]expand and pruning bad ai partition:  88%|████████▊ | 652/740 [02:18<00:04, 19.13it/s]expand and pruning bad ai partition:  89%|████████▊ | 656/740 [02:20<00:09,  8.91it/s]expand and pruning bad ai partition:  89%|████████▉ | 662/740 [02:20<00:08,  9.62it/s]expand and pruning bad ai partition:  90%|████████▉ | 664/740 [02:21<00:07,  9.54it/s]expand and pruning bad ai partition:  90%|█████████ | 667/740 [02:29<00:48,  1.50it/s]expand and pruning bad ai partition:  90%|█████████ | 669/740 [02:29<00:42,  1.68it/s]expand and pruning bad ai partition:  91%|█████████ | 675/740 [02:30<00:23,  2.81it/s]expand and pruning bad ai partition:  92%|█████████▏| 684/740 [02:30<00:11,  4.93it/s]expand and pruning bad ai partition:  94%|█████████▍| 696/740 [02:30<00:04,  8.86it/s]expand and pruning bad ai partition:  95%|█████████▌| 705/740 [02:30<00:02, 12.62it/s]expand and pruning bad ai partition:  96%|█████████▋| 713/740 [02:30<00:01, 16.71it/s]expand and pruning bad ai partition:  97%|█████████▋| 719/740 [02:32<00:02,  8.18it/s]expand and pruning bad ai partition:  98%|█████████▊| 725/740 [02:32<00:01, 10.46it/s]expand and pruning bad ai partition:  99%|█████████▊| 730/740 [02:33<00:01,  8.55it/s]expand and pruning bad ai partition:  99%|█████████▉| 734/740 [02:34<00:00,  6.76it/s]expand and pruning bad ai partition: 100%|██████████| 740/740 [02:34<00:00,  4.78it/s]
after expand and pruning bad ai partition: len(connected_partitions)=32
pruning bad partition(provide_output=True):   0%|          | 0/32 [00:00<?, ?it/s]pruning bad partition(provide_output=True): 100%|██████████| 32/32 [00:00<00:00, 2216.46it/s]
after pruning bad partition(provide_output=True): len(connected_partitions)=10
module {
  func.func @KeyFormer(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>) {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %14 = asuka.log %arg3 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %15 = asuka.neg %14 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %16 = asuka.add %7, %15 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %17 = asuka.div %16, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %18 = asuka.exp %17 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %19 = asuka.reduce(%18), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %20 = asuka.div %18, %19 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %21 = asuka.reduce(%20), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    return %13, %21 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>
  }
  asuka.kernel @KeyFormer_p0(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x4096xf32>) -> (tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>) {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %14 = asuka.log %arg3 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %15 = asuka.neg %14 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %16 = asuka.add %7, %15 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %17 = asuka.div %16, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %18 = asuka.exp %17 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %19 = asuka.reduce(%18), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %9, %13, %19 : tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>
  }
  asuka.kernel @KeyFormer_p1(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %13 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @KeyFormer_p2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x1xf32>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.div %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.convert %9, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %11 = asuka.dot %10, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %12 = asuka.permute %11, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %12 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @KeyFormer_p3(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>) {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %14 = asuka.log %arg3 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %15 = asuka.neg %14 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %16 = asuka.add %7, %15 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %17 = asuka.div %16, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %18 = asuka.exp %17 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %19 = asuka.reduce(%18), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %13, %19 : tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>
  }
  asuka.kernel @KeyFormer_p4(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x1xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.reduce(%7), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %8 : tensor<1x32x4096x1xf32>
  }
  asuka.kernel @KeyFormer_p5(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x32x4096x4096xf32>) -> (tensor<1x32x4096x1xf32>, tensor<1x32x4096x1xf32>) {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.reduce(%7), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %9 = asuka.log %arg2 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.neg %9 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.add %6, %10 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.div %11, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %13 = asuka.exp %12 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.reduce(%13), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %8, %14 : tensor<1x32x4096x1xf32>, tensor<1x32x4096x1xf32>
  }
  asuka.kernel @KeyFormer_p6(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x1xf32>, %arg4: tensor<1x32x4096x4096xf32>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>) {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.div %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.convert %9, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %11 = asuka.dot %10, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %12 = asuka.permute %11, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %13 = asuka.log %arg4 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.neg %13 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %15 = asuka.add %7, %14 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %16 = asuka.div %15, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %17 = asuka.exp %16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %18 = asuka.reduce(%17), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %12, %18 : tensor<1x4096x32x128xf16>, tensor<1x32x4096x1xf32>
  }
  asuka.kernel @KeyFormer_p7(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x32x4096x4096xf32>, %arg3: tensor<1x32x4096x1xf32>) -> tensor<1x32x4096xf32> {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.log %arg2 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.neg %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.add %6, %8 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.div %9, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.div %11, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %13 = asuka.reduce(%12), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    asuka.return %13 : tensor<1x32x4096xf32>
  }
  asuka.kernel @KeyFormer_p8(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %9, %13 : tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>
  }
  asuka.kernel @KeyFormer_p9(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32> {
    %cst = arith.constant dense<1.500000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.log %arg2 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.neg %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.add %6, %8 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.div %9, %cst : (tensor<1x32x4096x4096xf32>, tensor<1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.reduce(%11), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %12 : tensor<1x32x4096x1xf32>
  }
}
optimize KeyFormer_p0
optimize KeyFormer_p1
optimize KeyFormer_p2
optimize KeyFormer_p3
optimize KeyFormer_p4
optimize KeyFormer_p5
%0:2 = asuka.parallel_for <<UNKNOWN SSA VALUE>>, <<UNKNOWN SSA VALUE>>, <<UNKNOWN SSA VALUE>> {
^bb0(%arg0: index, %arg1: index, %arg2: index, %arg3: tensor<128x128xf64>, %arg4: tensor<4096x128xf64>, %arg5: tensor<128x4096xf64>):
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 0xFFF0000000000000 : f64
  %c1 = arith.constant 1 : index
  %cst_1 = arith.constant dense<0.96179668108622229> : tensor<1xf64>
  %cst_2 = arith.constant dense<1.4426950216293335> : tensor<1xf64>
  %cst_3 = arith.constant dense<1.131250e+01> : tensor<1xf64>
  %c0 = arith.constant 0 : index
  %0 = asuka.permute %arg4, dims = [1, 0] : (tensor<4096x128xf64>) -> tensor<128x4096xf64>
  %1 = asuka.div %arg3, %cst_3 : (tensor<128x128xf64>, tensor<1xf64>) -> tensor<128x128xf64>
  %2 = asuka.block_for lb = 0, ub = 4096, step = 128, args = [%0], dims = [1], init = [] {
  ^bb0(%arg6: index, %arg7: tensor<128x128xf64>):
    %12 = asuka.mask starts = [%arg1, %arg6], sizes = [128, 128], type = f64 {
    ^bb0(%arg8: index, %arg9: index):
      %15 = arith.addi %arg8, %c1 : index
      %16 = arith.cmpi ule, %15, %arg9 : index
      %17 = scf.if %16 -> (f64) {
        scf.yield %cst_0 : f64
      } else {
        scf.yield %cst : f64
      }
      asuka.mask_yield %17 : f64
    } : (index, index) -> tensor<128x128xf64>
    %13 = asuka.dot %1, %arg7 : (tensor<128x128xf64>, tensor<128x128xf64>) -> tensor<128x128xf64>
    %14 = asuka.add %13, %12 : (tensor<128x128xf64>, tensor<128x128xf64>) -> tensor<128x128xf64>
    asuka.block_yield block_outs = [%14], iter_outs = [] : tensor<128x128xf64>
  } : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %3 = asuka.mul %2, %cst_2 : (tensor<128x4096xf64>, tensor<1xf64>) -> tensor<128x4096xf64>
  %4 = asuka.exp2 %3 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %5 = asuka.reduce(%4), dim = 1, op =  ADD, keep_dim = true : (tensor<128x4096xf64>) -> tensor<128x1xf64>
  %6 = asuka.log %arg5 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %7 = asuka.neg %6 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %8 = asuka.add %2, %7 : (tensor<128x4096xf64>, tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %9 = asuka.mul %8, %cst_1 : (tensor<128x4096xf64>, tensor<1xf64>) -> tensor<128x4096xf64>
  %10 = asuka.exp2 %9 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %11 = asuka.reduce(%10), dim = 1, op =  ADD, keep_dim = true : (tensor<128x4096xf64>) -> tensor<128x1xf64>
  asuka.parallel_yield %5, %11 : tensor<128x1xf64>, tensor<128x1xf64>
} : (tensor<1x4096x32x128xf64>, tensor<1x4096x32x128xf64>, tensor<1x32x4096x4096xf64>) -> (tensor<1x32x4096x1xf64>, tensor<1x32x4096x1xf64>)
unreachable
%0:2 = asuka.parallel_for <<UNKNOWN SSA VALUE>>, <<UNKNOWN SSA VALUE>>, <<UNKNOWN SSA VALUE>> {
^bb0(%arg0: index, %arg1: index, %arg2: index, %arg3: tensor<128x128xf64>, %arg4: tensor<4096x128xf64>, %arg5: tensor<128x4096xf64>):
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 0xFFF0000000000000 : f64
  %c1 = arith.constant 1 : index
  %cst_1 = arith.constant dense<0.96179668108622229> : tensor<1xf64>
  %cst_2 = arith.constant dense<1.4426950216293335> : tensor<1xf64>
  %cst_3 = arith.constant dense<1.131250e+01> : tensor<1xf64>
  %0 = asuka.permute %arg4, dims = [1, 0] : (tensor<4096x128xf64>) -> tensor<128x4096xf64>
  %1 = asuka.div %arg3, %cst_3 : (tensor<128x128xf64>, tensor<1xf64>) -> tensor<128x128xf64>
  %2 = asuka.block_for lb = 0, ub = 4096, step = 128, args = [%0], dims = [1], init = [] {
  ^bb0(%arg6: index, %arg7: tensor<128x128xf64>):
    %12 = asuka.mask starts = [%arg1, %arg6], sizes = [128, 128], type = f64 {
    ^bb0(%arg8: index, %arg9: index):
      %15 = arith.addi %arg8, %c1 : index
      %16 = arith.cmpi ule, %15, %arg9 : index
      %17 = scf.if %16 -> (f64) {
        scf.yield %cst_0 : f64
      } else {
        scf.yield %cst : f64
      }
      asuka.mask_yield %17 : f64
    } : (index, index) -> tensor<128x128xf64>
    %13 = asuka.dot %1, %arg7 : (tensor<128x128xf64>, tensor<128x128xf64>) -> tensor<128x128xf64>
    %14 = asuka.add %13, %12 : (tensor<128x128xf64>, tensor<128x128xf64>) -> tensor<128x128xf64>
    asuka.block_yield block_outs = [%14], iter_outs = [] : tensor<128x128xf64>
  } : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %3 = asuka.mul %2, %cst_2 : (tensor<128x4096xf64>, tensor<1xf64>) -> tensor<128x4096xf64>
  %4 = asuka.exp2 %3 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %5 = asuka.reduce(%4), dim = 1, op =  ADD, keep_dim = true : (tensor<128x4096xf64>) -> tensor<128x1xf64>
  %6 = asuka.log %arg5 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %7 = asuka.neg %6 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %8 = asuka.add %2, %7 : (tensor<128x4096xf64>, tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %9 = asuka.mul %8, %cst_1 : (tensor<128x4096xf64>, tensor<1xf64>) -> tensor<128x4096xf64>
  %10 = asuka.exp2 %9 : (tensor<128x4096xf64>) -> tensor<128x4096xf64>
  %11 = asuka.reduce(%10), dim = 1, op =  ADD, keep_dim = true : (tensor<128x4096xf64>) -> tensor<128x1xf64>
  asuka.parallel_yield %5, %11 : tensor<128x1xf64>, tensor<128x1xf64>
} : (tensor<1x4096x32x128xf64>, tensor<1x4096x32x128xf64>, tensor<1x32x4096x4096xf64>) -> (tensor<1x32x4096x1xf64>, tensor<1x32x4096x1xf64>)
unreachable
error: failed to legalize operation 'asuka.dynamic_block_for' that was explicitly marked illegal
optimize KeyFormer_p5 failed
optimize KeyFormer_p6
optimize KeyFormer_p7
optimize KeyFormer_p8
optimize KeyFormer_p9
tuning time: 170.82245349884033 sec
Skip op: func.func
path: /tmp/tmp0d5uvapb.py
profiling...
out_str='[KeyFormer_p0] avg_ms: 3.4191513061523438\n[KeyFormer_p1] avg_ms: 0.5625138878822327\n[KeyFormer_p2] avg_ms: 0.5562514662742615\n[KeyFormer_p3] avg_ms: 3.3547539710998535\n[KeyFormer_p4] avg_ms: 0.5765530467033386\n[KeyFormer_p6] avg_ms: 3.12410569190979\n[KeyFormer_p7] avg_ms: 1.2857800722122192\n[KeyFormer_p8] avg_ms: 0.5605260729789734\n[KeyFormer_p9] avg_ms: 1.1525464057922363\n'
[KeyFormer_p0] avg_ms: 3.4191513061523438
[KeyFormer_p1] avg_ms: 0.5625138878822327
[KeyFormer_p2] avg_ms: 0.5562514662742615
[KeyFormer_p3] avg_ms: 3.3547539710998535
[KeyFormer_p4] avg_ms: 0.5765530467033386
[KeyFormer_p6] avg_ms: 3.12410569190979
[KeyFormer_p7] avg_ms: 1.2857800722122192
[KeyFormer_p8] avg_ms: 0.5605260729789734
[KeyFormer_p9] avg_ms: 1.1525464057922363
best_kernel:

KeyFormer_p8
KeyFormer_p9
KeyFormer_p7
best_time=2.998852550983429
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from typing import Callable, Any, Optional, Tuple

def bench_KeyFormer_p8():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  avg_ms = triton.testing.do_bench(lambda: KeyFormer_p8(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[KeyFormer_p8] avg_ms:', avg_ms)

def KeyFormer_p8(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  empty_ptr_4 = torch.empty(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  grid = (1, 32, 32)
  KeyFormer_p8_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_3
  tensor_6 = empty_ptr_4
  return tensor_5, tensor_6

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def KeyFormer_p8_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.275311e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 0
  const_12 = 1
  const_13 = 4096
  const_14 = 128
  mul_15 = pid_6 * const_14
  mul_16 = mul_15 * const_13
  mul_17 = pid_7 * const_14
  add_18 = mul_16 + mul_17
  block_ptr_19 = tl.make_block_ptr(
    base=arg_0 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_20 = tl.load(block_ptr_19)
  mul_21 = pid_7 * const_13
  add_22 = mul_15 + mul_21
  block_ptr_23 = tl.make_block_ptr(
    base=arg_3 + add_22,
    shape=(128, 1,),
    strides=(1, 1,),
    offsets=(0, 0,),
    block_shape=(128, 1,),
    order=(1, 0,),
  )
  block_ptr_24 = tl.make_block_ptr(
    base=arg_4 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  converted_25 = const_8
  mul_26 = block_load_20 * converted_25
  mul_26 = mul_26.to(tl.float16)
  zero_27 = tl.zeros([128, 128], dtype=tl.float32)
  zero_28 = tl.zeros([128, 1], dtype=tl.float32)
  add_29 = mul_15 + const_14
  block_ptr_30 = tl.make_block_ptr(
    base=arg_2 + mul_17,
    shape=(128, 4096,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_31 = tl.make_block_ptr(
    base=arg_1 + mul_17,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_32 in range(const_11, add_29, const_14):
    block_load_33 = tl.load(block_ptr_30)
    block_load_34 = tl.load(block_ptr_31)
    dot_35 = tl.dot(mul_26, block_load_33)
    where_36 = tl.zeros([128, 128], dtype=tl.float32)
    where_36 = tl.where(mul_15 + tl.arange(0, 128)[:, None] >= i_32 + tl.arange(0, 128)[None, :], where_36, float('-inf'))
    add_37 = dot_35 + where_36
    exp2_38 = tl.math.exp2(add_37)
    reduce_sum_39 = tl.sum(exp2_38, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_39 += zero_28
    converted_40 = exp2_38.to(tl.float16)
    dot_41 = tl.dot(converted_40, block_load_34)
    add_42 = zero_27 + dot_41
    block_advance_43 = tl.advance(block_ptr_30, (0, 128,))
    block_advance_44 = tl.advance(block_ptr_31, (128, 0,))
    block_ptr_30 = block_advance_43
    block_ptr_31 = block_advance_44
    zero_27 = add_42
    zero_28 = reduce_sum_39
  div_45 = zero_27 / zero_28
  converted_46 = div_45.to(tl.float16)
  block_store_47 = tl.store(block_ptr_23, zero_28)
  block_store_48 = tl.store(block_ptr_24, converted_46)

def bench_KeyFormer_p9():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 32, 4096, 4096, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: KeyFormer_p9(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[KeyFormer_p9] avg_ms:', avg_ms)

def KeyFormer_p9(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  grid = (1, 32, 32)
  KeyFormer_p9_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, autotune_key)
  tensor_4 = empty_ptr_3
  return tensor_4

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def KeyFormer_p9_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  autotune_key,
):
  pid_4 = tl.program_id(0)
  pid_5 = tl.program_id(1)
  pid_6 = tl.program_id(2)
  const_7 = 1.131250e+01
  const_8 = 9.617967e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 0
  const_12 = 16777216
  const_13 = 1
  const_14 = 4096
  const_15 = 128
  mul_16 = pid_5 * const_15
  mul_17 = mul_16 * const_14
  mul_18 = pid_6 * const_15
  add_19 = mul_17 + mul_18
  block_ptr_20 = tl.make_block_ptr(
    base=arg_0 + add_19,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_21 = tl.load(block_ptr_20)
  mul_22 = pid_6 * const_12
  add_23 = mul_17 + mul_22
  mul_24 = pid_6 * const_14
  add_25 = mul_16 + mul_24
  block_ptr_26 = tl.make_block_ptr(
    base=arg_3 + add_25,
    shape=(128, 1,),
    strides=(1, 1,),
    offsets=(0, 0,),
    block_shape=(128, 1,),
    order=(1, 0,),
  )
  converted_27 = const_7
  div_28 = block_load_21 / converted_27
  div_28 = div_28.to(tl.float16)
  zero_29 = tl.zeros([128, 1], dtype=tl.float32)
  add_30 = mul_16 + const_15
  block_ptr_31 = tl.make_block_ptr(
    base=arg_1 + mul_18,
    shape=(128, 4096,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_32 = tl.make_block_ptr(
    base=arg_2 + add_23,
    shape=(128, 4096,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_33 in range(const_11, add_30, const_15):
    block_load_34 = tl.load(block_ptr_31)
    block_load_35 = tl.load(block_ptr_32)
    log_36 = tl.math.log(block_load_35)
    neg_37 = -(log_36)
    mul_38 = neg_37 * const_8
    where_39 = tl.zeros([128, 128], dtype=tl.float32)
    where_39 = tl.where(mul_16 + tl.arange(0, 128)[:, None] >= i_33 + tl.arange(0, 128)[None, :], where_39, float('-inf'))
    converted_40 = const_8
    mul_41 = block_load_34 * converted_40
    mul_41 = mul_41.to(tl.float16)
    dot_42 = tl.dot(div_28, mul_41)
    add_43 = dot_42 + where_39
    add_44 = add_43 + mul_38
    exp2_45 = tl.math.exp2(add_44)
    reduce_sum_46 = tl.sum(exp2_45, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_46 += zero_29
    block_advance_47 = tl.advance(block_ptr_31, (0, 128,))
    block_advance_48 = tl.advance(block_ptr_32, (0, 128,))
    block_ptr_31 = block_advance_47
    block_ptr_32 = block_advance_48
    zero_29 = reduce_sum_46
  block_store_49 = tl.store(block_ptr_26, zero_29)

def bench_KeyFormer_p7():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 32, 4096, 4096, dtype=torch.float32, device=dev)
  rand_arg_3 = torch.randn(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: KeyFormer_p7(rand_arg_0, rand_arg_1, rand_arg_2, rand_arg_3))
  print('[KeyFormer_p7] avg_ms:', avg_ms)

def KeyFormer_p7(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  tensor_3 = arg3
  empty_ptr_4 = torch.empty(1, 32, 4096, dtype=torch.float32, device=dev)
  grid = (1, 32, 32)
  KeyFormer_p7_kernel[grid](tensor_0, tensor_1, tensor_2, tensor_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_4
  return tensor_5

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def KeyFormer_p7_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.131250e+01
  const_9 = 9.617967e-01
  const_10 = float('-inf')
  const_11 = 0.000000e+00
  const_12 = 16777216
  const_13 = 4096
  const_14 = 128
  const_15 = 1
  mul_16 = pid_6 * const_14
  mul_17 = pid_7 * const_14
  mul_18 = mul_17 * const_13
  add_19 = mul_16 + mul_18
  block_ptr_20 = tl.make_block_ptr(
    base=arg_1 + add_19,
    shape=(128, 128,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_load_21 = tl.load(block_ptr_20)
  mul_22 = pid_6 * const_12
  add_23 = mul_22 + mul_17
  mul_24 = pid_6 * const_13
  add_25 = mul_24 + mul_17
  block_ptr_26 = tl.make_block_ptr(
    base=arg_4 + add_25,
    shape=(128,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  converted_27 = const_9
  mul_28 = block_load_21 * converted_27
  mul_28 = mul_28.to(tl.float16)
  zero_29 = tl.zeros([128], dtype=tl.float32)
  block_ptr_30 = tl.make_block_ptr(
    base=arg_0 + add_19,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  add_31 = mul_18 + add_23
  block_ptr_32 = tl.make_block_ptr(
    base=arg_2 + add_31,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_ptr_33 = tl.make_block_ptr(
    base=arg_3 + add_25,
    shape=(4096,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  for i_34 in range(mul_17, const_13, const_14):
    block_load_35 = tl.load(block_ptr_30)
    block_load_36 = tl.load(block_ptr_32)
    block_load_37 = tl.load(block_ptr_33)
    log_38 = tl.math.log(block_load_36)
    neg_39 = -(log_38)
    mul_40 = neg_39 * const_9
    where_41 = tl.zeros([128, 128], dtype=tl.float32)
    where_41 = tl.where(i_34 + tl.arange(0, 128)[:, None] >= mul_17 + tl.arange(0, 128)[None, :], where_41, float('-inf'))
    converted_42 = const_8
    div_43 = block_load_35 / converted_42
    div_43 = div_43.to(tl.float16)
    dot_44 = tl.dot(div_43, mul_28)
    add_45 = dot_44 + where_41
    add_46 = add_45 + mul_40
    exp2_47 = tl.math.exp2(add_46)
    unsqueeze_48 = block_load_37[:, None]
    div_49 = exp2_47 / unsqueeze_48
    reduce_sum_50 = tl.sum(div_49, axis=0, keep_dims=False).to(tl.float32)
    reduce_sum_50 += zero_29
    block_advance_51 = tl.advance(block_ptr_30, (128, 0,))
    block_advance_52 = tl.advance(block_ptr_32, (128, 0,))
    block_advance_53 = tl.advance(block_ptr_33, (128,))
    block_ptr_30 = block_advance_51
    block_ptr_32 = block_advance_52
    block_ptr_33 = block_advance_53
    zero_29 = reduce_sum_50
  block_store_54 = tl.store(block_ptr_26, zero_29)

def KeyFormer(arg_0, arg_1, arg_2, arg_3):
  k0_out_0, k0_out_1 = KeyFormer_p8(arg_0, arg_2, arg_1)
  k1_out_0 = KeyFormer_p9(arg_0, arg_1, arg_3)
  k2_out_0 = KeyFormer_p7(arg_0, arg_1, arg_3, k1_out_0)
  return k0_out_1, k2_out_0

write code to /tmp/tmp_nlpq32f.py
weight_zoo_path='/home/ppopp25_ae/ppopp25_ae/weight_zoo.json'
hf_config.num_hidden_layers=32
data_path='/home/ppopp25_ae/ppopp25_ae/vcsum.jsonl'
token_ids.shape=torch.Size([1, 4096])
token_ids.grad=None
using safe tensor: files={'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors'}
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 194.04it/s]
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:02, 11.71it/s] 12%|█▎        | 4/32 [00:00<00:02, 12.03it/s] 19%|█▉        | 6/32 [00:00<00:02, 11.98it/s] 25%|██▌       | 8/32 [00:00<00:02, 11.78it/s] 31%|███▏      | 10/32 [00:00<00:01, 11.72it/s] 38%|███▊      | 12/32 [00:01<00:01, 11.68it/s] 44%|████▍     | 14/32 [00:01<00:01, 11.62it/s] 50%|█████     | 16/32 [00:01<00:01, 11.55it/s] 56%|█████▋    | 18/32 [00:01<00:01, 11.51it/s] 62%|██████▎   | 20/32 [00:01<00:01, 11.53it/s] 69%|██████▉   | 22/32 [00:01<00:00, 11.50it/s] 75%|███████▌  | 24/32 [00:02<00:00, 11.52it/s] 81%|████████▏ | 26/32 [00:02<00:00, 11.59it/s] 88%|████████▊ | 28/32 [00:02<00:00, 11.73it/s] 94%|█████████▍| 30/32 [00:02<00:00, 11.82it/s]100%|██████████| 32/32 [00:02<00:00, 11.82it/s]100%|██████████| 32/32 [00:02<00:00, 11.69it/s]
warmup start
warmup done
[our] avg 289.7238 ms, min 286.9945 ms, max 292.3703 ms (50 runs, 50 warmups, profiled)
