model='snapkv' system='our' seqlen=4096
input_names=['q', 'k', 'v']
output_names=['out', 'snapkv_score']
check=True
graph main_graph (
  %q[FLOAT16, 1x4096x32x128]
  %k[FLOAT16, 1x4096x32x128]
  %v[FLOAT16, 1x4096x32x128]
) {
  %/Constant_output_0 = Constant[value = <Tensor>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Trilu_output_0 = Trilu[upper = 1](%/Constant_output_0, %/Constant_1_output_0)
  %/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%q)
  %/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%v)
  %/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%k)
  %/MatMul_output_0 = MatMul(%/Transpose_output_0, %/Transpose_2_output_0)
  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_output_0 = Div(%/MatMul_output_0, %/Constant_2_output_0)
  %/Add_output_0 = Add(%/Div_output_0, %/Trilu_output_0)
  %/Cast_output_0 = Cast[to = 1](%/Add_output_0)
  %/Softmax_output_0 = Softmax[axis = -1](%/Cast_output_0)
  %/Cast_1_output_0 = Cast[to = 10](%/Softmax_output_0)
  %/MatMul_1_output_0 = MatMul(%/Cast_1_output_0, %/Transpose_1_output_0)
  %/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/MatMul_1_output_0)
  %onnx::ReduceSum_21 = Constant[value = <Tensor>]()
  %/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/Softmax_output_0, %onnx::ReduceSum_21)
  %/AveragePool_output_0 = AveragePool[ceil_mode = 0, count_include_pad = 1, kernel_shape = [5], pads = [2, 2], strides = [1]](%/ReduceSum_output_0)
  %/Constant_3_output_0 = Constant[value = <Tensor>]()
  %out = Reshape[allowzero = 0](%/Transpose_3_output_0, %/Constant_3_output_0)
  %/Constant_4_output_0 = Constant[value = <Tensor>]()
  %snapkv_score = Reshape[allowzero = 0](%/AveragePool_output_0, %/Constant_4_output_0)
  return %out, %snapkv_score
}
/home/ppopp25_ae/ppopp25_ae/3rd/asuka/python/asuka/translate.py:217: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  operand_torch = torch.from_numpy(operand)
module {
  func.func @SnapKV(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>) {
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.softmax %7, dim = -1 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.convert %8, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.dot %9, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %11 = asuka.permute %10, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %12 = asuka.reduce(%8), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    %13 = asuka.avg_pool %12, kernel_size = [5], stride = [1], padding = [2], ceil_mode = false, count_include_pad = true : (tensor<1x32x4096xf32>) -> tensor<1x32x4096xf32>
    %14 = asuka.reshape %11 : (tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16>
    %15 = asuka.reshape %13 : (tensor<1x32x4096xf32>) -> tensor<1x32x4096xf32>
    return %14, %15 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>
  }
}
idx=0 arith.constant
idx=1 asuka.trilu
idx=2 asuka.permute
idx=3 asuka.permute
idx=4 asuka.permute
idx=5 asuka.dot
idx=6 asuka.div
idx=7 asuka.add
idx=8 asuka.convert
idx=9 asuka.exp
idx=10 asuka.reduce
idx=11 asuka.div
idx=12 asuka.convert
idx=13 asuka.dot
idx=14 asuka.permute
idx=15 asuka.reduce
idx=16 asuka.avg_pool
len(connected_partitions)=1026
pruning bad partition(provide_output=False):   0%|          | 0/1026 [00:00<?, ?it/s]pruning bad partition(provide_output=False):  22%|██▏       | 225/1026 [00:00<00:00, 2245.28it/s]pruning bad partition(provide_output=False):  44%|████▍     | 454/1026 [00:00<00:00, 2264.64it/s]pruning bad partition(provide_output=False):  67%|██████▋   | 690/1026 [00:00<00:00, 2307.09it/s]pruning bad partition(provide_output=False):  90%|████████▉ | 921/1026 [00:00<00:00, 2276.07it/s]pruning bad partition(provide_output=False): 100%|██████████| 1026/1026 [00:00<00:00, 2281.60it/s]
after pruning bad partition: len(connected_partitions)=101
pruning bad para partition:   0%|          | 0/101 [00:00<?, ?it/s]pruning bad para partition:  64%|██████▍   | 65/101 [00:00<00:00, 634.52it/s]pruning bad para partition: 100%|██████████| 101/101 [00:00<00:00, 643.01it/s]
after pruning bad para partition: len(connected_partitions)=55
max_metric:  536870912.0
tall_and_thin tensors: 8
expand and pruning bad ai partition:   0%|          | 0/55 [00:00<?, ?it/s]expand and pruning bad ai partition:  56%|█████▋    | 31/55 [00:00<00:00, 53.23it/s]expand and pruning bad ai partition:  95%|█████████▍| 52/55 [00:00<00:00, 57.68it/s]expand and pruning bad ai partition: 100%|██████████| 55/55 [00:00<00:00, 59.10it/s]
after expand and pruning bad ai partition: len(connected_partitions)=10
pruning bad partition(provide_output=True):   0%|          | 0/10 [00:00<?, ?it/s]pruning bad partition(provide_output=True): 100%|██████████| 10/10 [00:00<00:00, 2254.52it/s]
after pruning bad partition(provide_output=True): len(connected_partitions)=5
module {
  func.func @SnapKV(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %14 = asuka.reduce(%10), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    %15 = asuka.avg_pool %14, kernel_size = [5], stride = [1], padding = [2], ceil_mode = false, count_include_pad = true : (tensor<1x32x4096xf32>) -> tensor<1x32x4096xf32>
    return %13, %15 : tensor<1x4096x32x128xf16>, tensor<1x32x4096xf32>
  }
  asuka.kernel @SnapKV_p0(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x1xf32>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.div %8, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.convert %9, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %11 = asuka.dot %10, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %12 = asuka.permute %11, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %12 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @SnapKV_p1(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %9, %13 : tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>
  }
  asuka.kernel @SnapKV_p2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.exp %7 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %13 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @SnapKV_p3(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x1xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.reduce(%7), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %8 : tensor<1x32x4096x1xf32>
  }
  asuka.kernel @SnapKV_p4(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x32x4096x1xf32>) -> tensor<1x32x4096xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %7 = asuka.exp %6 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %8 = asuka.div %7, %arg2 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %9 = asuka.reduce(%8), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096xf32>
    asuka.return %9 : tensor<1x32x4096xf32>
  }
}
optimize SnapKV_p0
optimize SnapKV_p1
optimize SnapKV_p2
optimize SnapKV_p3
optimize SnapKV_p4
tuning time: 2.6205499172210693 sec
Skip op: func.func
uncovered ops: asuka.avg_pool
path: /tmp/tmpfskz67h7.py
profiling...
out_str='[SnapKV_p0] avg_ms: 0.5512588024139404\n[SnapKV_p1] avg_ms: 0.5533682107925415\n[SnapKV_p2] avg_ms: 0.5552537441253662\n[SnapKV_p3] avg_ms: 0.5808859467506409\n[SnapKV_p4] avg_ms: 0.6953528523445129\n[SnapKV_uncovered_0] avg_ms: 0.00885848794132471\n'
[SnapKV_p0] avg_ms: 0.5512588024139404
[SnapKV_p1] avg_ms: 0.5533682107925415
[SnapKV_p2] avg_ms: 0.5552537441253662
[SnapKV_p3] avg_ms: 0.5808859467506409
[SnapKV_p4] avg_ms: 0.6953528523445129
[SnapKV_uncovered_0] avg_ms: 0.00885848794132471
best_kernel:

SnapKV_p1
SnapKV_p4
SnapKV_uncovered_0
best_time=1.2575795510783792
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from typing import Callable, Any, Optional, Tuple

def bench_SnapKV_p1():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  avg_ms = triton.testing.do_bench(lambda: SnapKV_p1(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[SnapKV_p1] avg_ms:', avg_ms)

def SnapKV_p1(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  empty_ptr_4 = torch.empty(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  grid = (1, 32, 32)
  SnapKV_p1_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_3
  tensor_6 = empty_ptr_4
  return tensor_5, tensor_6

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def SnapKV_p1_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.275311e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 0
  const_12 = 1
  const_13 = 4096
  const_14 = 128
  mul_15 = pid_6 * const_14
  mul_16 = mul_15 * const_13
  mul_17 = pid_7 * const_14
  add_18 = mul_16 + mul_17
  block_ptr_19 = tl.make_block_ptr(
    base=arg_0 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_20 = tl.load(block_ptr_19)
  mul_21 = pid_7 * const_13
  add_22 = mul_15 + mul_21
  block_ptr_23 = tl.make_block_ptr(
    base=arg_3 + add_22,
    shape=(128, 1,),
    strides=(1, 1,),
    offsets=(0, 0,),
    block_shape=(128, 1,),
    order=(1, 0,),
  )
  block_ptr_24 = tl.make_block_ptr(
    base=arg_4 + add_18,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  converted_25 = const_8
  mul_26 = block_load_20 * converted_25
  mul_26 = mul_26.to(tl.float16)
  zero_27 = tl.zeros([128, 128], dtype=tl.float32)
  zero_28 = tl.zeros([128, 1], dtype=tl.float32)
  add_29 = mul_15 + const_14
  block_ptr_30 = tl.make_block_ptr(
    base=arg_2 + mul_17,
    shape=(128, 4096,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_31 = tl.make_block_ptr(
    base=arg_1 + mul_17,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_32 in range(const_11, add_29, const_14):
    block_load_33 = tl.load(block_ptr_30)
    block_load_34 = tl.load(block_ptr_31)
    dot_35 = tl.dot(mul_26, block_load_33)
    where_36 = tl.zeros([128, 128], dtype=tl.float32)
    where_36 = tl.where(mul_15 + tl.arange(0, 128)[:, None] >= i_32 + tl.arange(0, 128)[None, :], where_36, float('-inf'))
    add_37 = dot_35 + where_36
    exp2_38 = tl.math.exp2(add_37)
    reduce_sum_39 = tl.sum(exp2_38, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_39 += zero_28
    converted_40 = exp2_38.to(tl.float16)
    dot_41 = tl.dot(converted_40, block_load_34)
    add_42 = zero_27 + dot_41
    block_advance_43 = tl.advance(block_ptr_30, (0, 128,))
    block_advance_44 = tl.advance(block_ptr_31, (128, 0,))
    block_ptr_30 = block_advance_43
    block_ptr_31 = block_advance_44
    zero_27 = add_42
    zero_28 = reduce_sum_39
  div_45 = zero_27 / zero_28
  converted_46 = div_45.to(tl.float16)
  block_store_47 = tl.store(block_ptr_23, zero_28)
  block_store_48 = tl.store(block_ptr_24, converted_46)

def bench_SnapKV_p4():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 32, 4096, 1, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: SnapKV_p4(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[SnapKV_p4] avg_ms:', avg_ms)

def SnapKV_p4(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 4096, dtype=torch.float32, device=dev)
  grid = (1, 32, 32)
  SnapKV_p4_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, autotune_key)
  tensor_4 = empty_ptr_3
  return tensor_4

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def SnapKV_p4_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  autotune_key,
):
  pid_4 = tl.program_id(0)
  pid_5 = tl.program_id(1)
  pid_6 = tl.program_id(2)
  const_7 = 1.275311e-01
  const_8 = float('-inf')
  const_9 = 0.000000e+00
  const_10 = 4096
  const_11 = 128
  const_12 = 1
  mul_13 = pid_6 * const_11
  mul_14 = pid_5 * const_11
  mul_15 = mul_14 * const_10
  add_16 = mul_15 + mul_13
  block_ptr_17 = tl.make_block_ptr(
    base=arg_1 + add_16,
    shape=(128, 128,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_load_18 = tl.load(block_ptr_17)
  mul_19 = pid_6 * const_10
  add_20 = mul_14 + mul_19
  block_ptr_21 = tl.make_block_ptr(
    base=arg_3 + add_20,
    shape=(128,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  converted_22 = const_7
  mul_23 = block_load_18 * converted_22
  mul_23 = mul_23.to(tl.float16)
  zero_24 = tl.zeros([128], dtype=tl.float32)
  block_ptr_25 = tl.make_block_ptr(
    base=arg_0 + add_16,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_ptr_26 = tl.make_block_ptr(
    base=arg_2 + add_20,
    shape=(4096,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  for i_27 in range(mul_14, const_10, const_11):
    block_load_28 = tl.load(block_ptr_25)
    block_load_29 = tl.load(block_ptr_26)
    where_30 = tl.zeros([128, 128], dtype=tl.float32)
    where_30 = tl.where(i_27 + tl.arange(0, 128)[:, None] >= mul_14 + tl.arange(0, 128)[None, :], where_30, float('-inf'))
    dot_31 = tl.dot(block_load_28, mul_23)
    add_32 = dot_31 + where_30
    exp2_33 = tl.math.exp2(add_32)
    unsqueeze_34 = block_load_29[:, None]
    div_35 = exp2_33 / unsqueeze_34
    reduce_sum_36 = tl.sum(div_35, axis=0, keep_dims=False).to(tl.float32)
    reduce_sum_36 += zero_24
    block_advance_37 = tl.advance(block_ptr_25, (128, 0,))
    block_advance_38 = tl.advance(block_ptr_26, (128,))
    block_ptr_25 = block_advance_37
    block_ptr_26 = block_advance_38
    zero_24 = reduce_sum_36
  block_store_39 = tl.store(block_ptr_21, zero_24)

def bench_SnapKV_uncovered_0():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 32, 4096, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: SnapKV_uncovered_0(rand_arg_0))
  print('[SnapKV_uncovered_0] avg_ms:', avg_ms)

def SnapKV_uncovered_0(arg0: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  avg_pool_0 = F.avg_pool1d(arg0, kernel_size=5, stride=1, padding=2, ceil_mode=False, count_include_pad=True)
  return avg_pool_0

def SnapKV(arg_0, arg_1, arg_2):
  k0_out_0, k0_out_1 = SnapKV_p1(arg_0, arg_2, arg_1)
  k1_out_0 = SnapKV_p4(arg_0, arg_1, k0_out_0)
  k2_out_0 = SnapKV_uncovered_0(k1_out_0)
  return k0_out_1, k2_out_0

write code to /tmp/tmpntrih67h.py
start_peak_mem_mib=128.0
end_peak_mem_mib=417.0
gflops=137.438953472
mib=417.0
checking our...
out loss: {'abs_max': 0.001953125, 'abs_mean': 2.7582870298914486e-05, 'rel_max': inf, 'rel_mean': inf, 'nz_rel_max': 994.0, 'nz_rel_mean': 0.006222191583071725, 'mse': 2.319242889984175e-09, 'rmse': 4.8158518353290055e-05}
snapkv_score loss: {'abs_max': 0.0006375312805175781, 'abs_mean': 1.5570384255148007e-05, 'rel_max': 0.0007162224112838735, 'rel_mean': 1.8583690329502087e-05, 'nz_rel_max': 0.0007162224112838735, 'nz_rel_mean': 1.8583690329502087e-05, 'mse': 7.855671463333161e-10, 'rmse': 2.802797078515168e-05}
warmup start
warmup done
[our] avg 1.2767 ms, min 1.2646 ms, max 1.3509 ms, 107649.06672332839 gflops/s (10 runs, 100 warmups, profiled)
