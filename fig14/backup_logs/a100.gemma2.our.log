srun: job 181835 queued and waiting for resources
srun: job 181835 has been allocated resources
model='gemma2' system='our' seqlen=4096
input_names=['q', 'k', 'v']
output_names=['out']
check=True
graph main_graph (
  %q[FLOAT16, 1x4096x32x128]
  %k[FLOAT16, 1x4096x32x128]
  %v[FLOAT16, 1x4096x32x128]
) {
  %/Constant_output_0 = Constant[value = <Tensor>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Trilu_output_0 = Trilu[upper = 1](%/Constant_output_0, %/Constant_1_output_0)
  %/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%q)
  %/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%v)
  %/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%k)
  %/MatMul_output_0 = MatMul(%/Transpose_output_0, %/Transpose_2_output_0)
  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_output_0 = Div(%/MatMul_output_0, %/Constant_2_output_0)
  %/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_1_output_0 = Div(%/Div_output_0, %/Constant_3_output_0)
  %/Tanh_output_0 = Tanh(%/Div_1_output_0)
  %/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Mul_output_0 = Mul(%/Tanh_output_0, %/Constant_4_output_0)
  %/Add_output_0 = Add(%/Mul_output_0, %/Trilu_output_0)
  %/Cast_output_0 = Cast[to = 1](%/Add_output_0)
  %/Softmax_output_0 = Softmax[axis = -1](%/Cast_output_0)
  %/Cast_1_output_0 = Cast[to = 10](%/Softmax_output_0)
  %/MatMul_1_output_0 = MatMul(%/Cast_1_output_0, %/Transpose_1_output_0)
  %/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/MatMul_1_output_0)
  %/Constant_5_output_0 = Constant[value = <Tensor>]()
  %out = Reshape[allowzero = 0](%/Transpose_3_output_0, %/Constant_5_output_0)
  return %out
}
/home/ppopp25_ae/ppopp25_ae/3rd/asuka/python/asuka/translate.py:217: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  operand_torch = torch.from_numpy(operand)
module {
  func.func @Gemma2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %cst_0 = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %6 = asuka.div %5, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.tanh %6 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %cst_1 = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %8 = asuka.mul %7, %cst_1 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.add %8, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.convert %9, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.softmax %10, dim = -1 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.convert %11, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %13 = asuka.dot %12, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %14 = asuka.permute %13, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    %15 = asuka.reshape %14 : (tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16>
    return %15 : tensor<1x4096x32x128xf16>
  }
}
idx=0 arith.constant
idx=1 arith.constant
idx=2 asuka.trilu
idx=3 asuka.permute
idx=4 asuka.permute
idx=5 asuka.permute
idx=6 asuka.dot
idx=7 asuka.div
idx=8 asuka.div
idx=9 asuka.tanh
idx=10 asuka.mul
idx=11 asuka.add
idx=12 asuka.convert
idx=13 asuka.exp
idx=14 asuka.reduce
idx=15 asuka.div
idx=16 asuka.convert
idx=17 asuka.dot
idx=18 asuka.permute
len(connected_partitions)=1370
pruning bad partition(provide_output=False):   0%|          | 0/1370 [00:00<?, ?it/s]pruning bad partition(provide_output=False):  11%|█▏        | 155/1370 [00:00<00:00, 1547.56it/s]pruning bad partition(provide_output=False):  23%|██▎       | 312/1370 [00:00<00:00, 1555.16it/s]pruning bad partition(provide_output=False):  35%|███▍      | 475/1370 [00:00<00:00, 1587.43it/s]pruning bad partition(provide_output=False):  46%|████▋     | 637/1370 [00:00<00:00, 1598.60it/s]pruning bad partition(provide_output=False):  58%|█████▊    | 797/1370 [00:00<00:00, 1598.62it/s]pruning bad partition(provide_output=False):  71%|███████   | 967/1370 [00:00<00:00, 1630.90it/s]pruning bad partition(provide_output=False):  83%|████████▎ | 1131/1370 [00:00<00:00, 1627.46it/s]pruning bad partition(provide_output=False):  94%|█████████▍| 1294/1370 [00:00<00:00, 1628.05it/s]pruning bad partition(provide_output=False): 100%|██████████| 1370/1370 [00:00<00:00, 1615.44it/s]
after pruning bad partition: len(connected_partitions)=139
pruning bad para partition:   0%|          | 0/139 [00:00<?, ?it/s]pruning bad para partition:  30%|███       | 42/139 [00:00<00:00, 412.10it/s]pruning bad para partition:  65%|██████▍   | 90/139 [00:00<00:00, 451.48it/s]pruning bad para partition:  98%|█████████▊| 136/139 [00:00<00:00, 440.84it/s]pruning bad para partition: 100%|██████████| 139/139 [00:00<00:00, 440.28it/s]
after pruning bad para partition: len(connected_partitions)=127
max_metric:  536870912.0
tall_and_thin tensors: 7
expand and pruning bad ai partition:   0%|          | 0/127 [00:00<?, ?it/s]expand and pruning bad ai partition:   9%|▊         | 11/127 [00:00<00:01, 69.10it/s]expand and pruning bad ai partition:  20%|█▉        | 25/127 [00:02<00:10, 10.14it/s]expand and pruning bad ai partition:  23%|██▎       | 29/127 [00:02<00:09,  9.96it/s]expand and pruning bad ai partition:  25%|██▌       | 32/127 [00:06<00:28,  3.31it/s]expand and pruning bad ai partition:  28%|██▊       | 35/127 [00:06<00:26,  3.44it/s]expand and pruning bad ai partition:  36%|███▌      | 46/127 [00:07<00:11,  6.88it/s]expand and pruning bad ai partition:  54%|█████▍    | 69/127 [00:07<00:03, 15.69it/s]expand and pruning bad ai partition:  59%|█████▉    | 75/127 [00:07<00:02, 17.47it/s]expand and pruning bad ai partition:  76%|███████▌  | 96/127 [00:07<00:00, 31.18it/s]expand and pruning bad ai partition:  83%|████████▎ | 106/127 [00:07<00:00, 36.49it/s]expand and pruning bad ai partition:  91%|█████████▏| 116/127 [00:08<00:00, 37.08it/s]expand and pruning bad ai partition: 100%|██████████| 127/127 [00:08<00:00, 15.72it/s]
after expand and pruning bad ai partition: len(connected_partitions)=16
pruning bad partition(provide_output=True):   0%|          | 0/16 [00:00<?, ?it/s]pruning bad partition(provide_output=True): 100%|██████████| 16/16 [00:00<00:00, 1482.32it/s]
after pruning bad partition(provide_output=True): len(connected_partitions)=4
module {
  func.func @Gemma2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.div %5, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.tanh %6 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %8 = asuka.mul %7, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.add %8, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.convert %9, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.reduce(%11), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %13 = asuka.div %11, %12 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.convert %13, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %15 = asuka.dot %14, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %16 = asuka.permute %15, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    return %16 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Gemma2_p0(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>, %arg3: tensor<1x32x4096x1xf32>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.div %5, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.tanh %6 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %8 = asuka.mul %7, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.add %8, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.convert %9, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.div %11, %arg3 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %13 = asuka.convert %12, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %14 = asuka.dot %13, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %15 = asuka.permute %14, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %15 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Gemma2_p1(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x1xf32> {
    %cst = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %4 = asuka.div %3, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.tanh %5 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.mul %6, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %8 = asuka.add %7, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.convert %8, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %10 = asuka.exp %9 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.reduce(%10), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    asuka.return %11 : tensor<1x32x4096x1xf32>
  }
  asuka.kernel @Gemma2_p2(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> tensor<1x4096x32x128xf16> {
    %cst = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.div %5, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.tanh %6 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %8 = asuka.mul %7, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.add %8, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.convert %9, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.reduce(%11), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %13 = asuka.div %11, %12 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.convert %13, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %15 = asuka.dot %14, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %16 = asuka.permute %15, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %16 : tensor<1x4096x32x128xf16>
  }
  asuka.kernel @Gemma2_p3(%arg0: tensor<1x4096x32x128xf16>, %arg1: tensor<1x4096x32x128xf16>, %arg2: tensor<1x4096x32x128xf16>) -> (tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>) {
    %cst = arith.constant dense<5.000000e+01> : tensor<1xf16>
    %cst_0 = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [4096, 4096], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x4096x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x4096x32x128xf16>) -> tensor<1x32x128x4096xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x4096x128xf16>, tensor<1x32x128x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %5 = asuka.div %4, %cst_0 : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %6 = asuka.div %5, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %7 = asuka.tanh %6 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %8 = asuka.mul %7, %cst : (tensor<1x32x4096x4096xf16>, tensor<1xf16>) -> tensor<1x32x4096x4096xf16>
    %9 = asuka.add %8, %0 : (tensor<1x32x4096x4096xf16>, tensor<4096x4096xf16>) -> tensor<1x32x4096x4096xf16>
    %10 = asuka.convert %9, type = f32 : (tensor<1x32x4096x4096xf16>) -> tensor<1x32x4096x4096xf32>
    %11 = asuka.exp %10 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf32>
    %12 = asuka.reduce(%11), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x1xf32>
    %13 = asuka.div %11, %12 : (tensor<1x32x4096x4096xf32>, tensor<1x32x4096x1xf32>) -> tensor<1x32x4096x4096xf32>
    %14 = asuka.convert %13, type = f16 : (tensor<1x32x4096x4096xf32>) -> tensor<1x32x4096x4096xf16>
    %15 = asuka.dot %14, %2 : (tensor<1x32x4096x4096xf16>, tensor<1x32x4096x128xf16>) -> tensor<1x32x4096x128xf16>
    %16 = asuka.permute %15, dims = [0, 2, 1, 3] : (tensor<1x32x4096x128xf16>) -> tensor<1x4096x32x128xf16>
    asuka.return %12, %16 : tensor<1x32x4096x1xf32>, tensor<1x4096x32x128xf16>
  }
}
optimize Gemma2_p0
optimize Gemma2_p1
optimize Gemma2_p2
optimize Gemma2_p3
tuning time: 10.854129076004028 sec
Skip op: func.func
path: /tmp/tmpcmu1s66m.py
profiling...
out_str='[Gemma2_p0] avg_ms: 1.0153635740280151\n[Gemma2_p1] avg_ms: 0.7371003031730652\n[Gemma2_p2] avg_ms: 1.0042669773101807\n[Gemma2_p3] avg_ms: 1.0136736631393433\n'
[Gemma2_p0] avg_ms: 1.0153635740280151
[Gemma2_p1] avg_ms: 0.7371003031730652
[Gemma2_p2] avg_ms: 1.0042669773101807
[Gemma2_p3] avg_ms: 1.0136736631393433
best_kernel:

Gemma2_p2
best_time=1.0042669773101807
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from typing import Callable, Any, Optional, Tuple

def bench_Gemma2_p2():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  avg_ms = triton.testing.do_bench(lambda: Gemma2_p2(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[Gemma2_p2] avg_ms:', avg_ms)

def Gemma2_p2(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 4096, 32, 128, dtype=torch.float16, device=dev)
  grid = (1, 32, 32)
  Gemma2_p2_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, autotune_key)
  tensor_4 = empty_ptr_3
  return tensor_4

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def Gemma2_p2_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  autotune_key,
):
  pid_4 = tl.program_id(0)
  pid_5 = tl.program_id(1)
  pid_6 = tl.program_id(2)
  const_7 = 1.131250e+01
  const_8 = 5.000000e+01
  const_9 = 1.442695e+00
  const_10 = float('-inf')
  const_11 = 0.000000e+00
  const_12 = 0
  const_13 = 1
  const_14 = 4096
  const_15 = 128
  mul_16 = pid_5 * const_15
  mul_17 = mul_16 * const_14
  mul_18 = pid_6 * const_15
  add_19 = mul_17 + mul_18
  block_ptr_20 = tl.make_block_ptr(
    base=arg_0 + add_19,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_21 = tl.load(block_ptr_20)
  block_ptr_22 = tl.make_block_ptr(
    base=arg_3 + add_19,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  converted_23 = const_7
  div_24 = block_load_21 / converted_23
  div_24 = div_24.to(tl.float16)
  converted_25 = const_8
  div_26 = div_24 / converted_25
  div_26 = div_26.to(tl.float16)
  zero_27 = tl.zeros([128, 128], dtype=tl.float32)
  zero_28 = tl.zeros([128, 1], dtype=tl.float32)
  add_29 = mul_16 + const_15
  block_ptr_30 = tl.make_block_ptr(
    base=arg_2 + mul_18,
    shape=(128, 4096,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_31 = tl.make_block_ptr(
    base=arg_1 + mul_18,
    shape=(4096, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_32 in range(const_12, add_29, const_15):
    block_load_33 = tl.load(block_ptr_30)
    block_load_34 = tl.load(block_ptr_31)
    dot_35 = tl.dot(div_26, block_load_33)
    where_36 = tl.zeros([128, 128], dtype=tl.float32)
    where_36 = tl.where(mul_16 + tl.arange(0, 128)[:, None] >= i_32 + tl.arange(0, 128)[None, :], where_36, float('-inf'))
    tanh_37 = tl.inline_asm_elementwise(
      asm='tanh.approx.f32 $0, $1;',
      constraints=('=r,r'),
      args=[dot_35],
      dtype=(tl.float32,),
      is_pure=True,
      pack=1,
    )
    mul_38 = tanh_37 * const_8
    mul_39 = mul_38 * const_9
    add_40 = mul_39 + where_36
    exp2_41 = tl.math.exp2(add_40)
    reduce_sum_42 = tl.sum(exp2_41, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_42 += zero_28
    converted_43 = exp2_41.to(tl.float16)
    dot_44 = tl.dot(converted_43, block_load_34)
    add_45 = zero_27 + dot_44
    block_advance_46 = tl.advance(block_ptr_30, (0, 128,))
    block_advance_47 = tl.advance(block_ptr_31, (128, 0,))
    block_ptr_30 = block_advance_46
    block_ptr_31 = block_advance_47
    zero_27 = add_45
    zero_28 = reduce_sum_42
  div_48 = zero_27 / zero_28
  converted_49 = div_48.to(tl.float16)
  block_store_50 = tl.store(block_ptr_22, converted_49)

def Gemma2(arg_0, arg_1, arg_2):
  k0_out_0 = Gemma2_p2(arg_0, arg_2, arg_1)
  return k0_out_0

write code to /tmp/tmp2zp68hk4.py
start_peak_mem_mib=104.125
end_peak_mem_mib=392.125
gflops=137.438953472
mib=392.125
checking our...
out loss: {'abs_max': 0.00341796875, 'abs_mean': 4.1480680806671444e-05, 'rel_max': inf, 'rel_mean': inf, 'nz_rel_max': 1192.0, 'nz_rel_mean': 0.009387028942277534, 'mse': 4.723735561718741e-09, 'rmse': 6.872943737379742e-05}
warmup start
warmup done
[our] avg 1.1011 ms, min 1.0929 ms, max 1.1203 ms, 124815.57914981563 gflops/s (10 runs, 100 warmups, profiled)
