srun: job 5082 queued and waiting for resources
srun: job 5082 has been allocated resources
[11/26/2024-13:35:38] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 256, GPU 1061 (MiB)
[11/26/2024-13:35:46] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4688, GPU +1290, now: CPU 5100, GPU 2351 (MiB)
[11/26/2024-13:35:46] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.
[11/26/2024-13:35:46] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[11/26/2024-13:35:46] [TRT] [I] Compiler backend is used during engine build.
[11/26/2024-13:35:48] [TRT] [I] Detected 3 inputs and 2 output network tensors.
[11/26/2024-13:35:48] [TRT] [I] Total Host Persistent Memory: 80 bytes
[11/26/2024-13:35:48] [TRT] [I] Total Device Persistent Memory: 0 bytes
[11/26/2024-13:35:48] [TRT] [I] Max Scratch Memory: 226492416 bytes
[11/26/2024-13:35:48] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.
[11/26/2024-13:35:48] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.011844ms to assign 1 blocks to 1 nodes requiring 226492416 bytes.
[11/26/2024-13:35:48] [TRT] [I] Total Activation Memory: 226492416 bytes
[11/26/2024-13:35:48] [TRT] [I] Total Weights Memory: 2097536 bytes
[11/26/2024-13:35:48] [TRT] [I] Compiler backend is used during engine execution.
[11/26/2024-13:35:48] [TRT] [I] Engine generation completed in 2.02106 seconds.
[11/26/2024-13:35:48] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 256 MiB
[11/26/2024-13:35:49] [TRT] [I] Loaded engine size: 2 MiB
[11/26/2024-13:35:49] [TRT] [I] [MS] Running engine with multi stream info
[11/26/2024-13:35:49] [TRT] [I] [MS] Number of aux streams is 2
[11/26/2024-13:35:49] [TRT] [I] [MS] Number of total worker streams is 3
[11/26/2024-13:35:49] [TRT] [I] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[11/26/2024-13:35:49] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +216, now: CPU 0, GPU 218 (MiB)
model='h2o' system='tensorrt' seqlen=1024
input_names=['q', 'k', 'v']
output_names=['out', 'h2o_score']
check=True
tensorrt_llm not found
start_peak_mem_mib=56.0
[11/26/2024-13:35:49] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.
end_peak_mem_mib=64.125
gflops=8.589934592
mib=64.125
checking tensorrt...
out loss: {'abs_max': 0.0048828125, 'abs_mean': 8.673367874223459e-05, 'rel_max': 4909.0, 'rel_mean': 0.011081034726278235, 'nz_rel_max': 4909.0, 'nz_rel_mean': 0.011081034726278235, 'mse': 1.712224750453955e-08, 'rmse': 0.00013085200611583894}
h2o_score loss: {'abs_max': 0.0015172958374023438, 'abs_mean': 5.989829503727506e-05, 'rel_max': 0.0021001861179179167, 'rel_mean': 7.824630960881688e-05, 'nz_rel_max': 0.0021001861179179167, 'nz_rel_mean': 7.824630960881688e-05, 'mse': 1.1766030731395659e-08, 'rmse': 0.00010847133598972431}
warmup start
warmup done
[tensorrt] avg 0.5978 ms, min 0.5867 ms, max 0.6447 ms, 14368.413566884934 gflops/s (10 runs, 100 warmups, profiled)
