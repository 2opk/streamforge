srun: job 5091 queued and waiting for resources
srun: job 5091 has been allocated resources
model='h2o' system='our' seqlen=2048
input_names=['q', 'k', 'v']
output_names=['out', 'h2o_score']
check=True
graph main_graph (
  %q[FLOAT16, 1x2048x32x128]
  %k[FLOAT16, 1x2048x32x128]
  %v[FLOAT16, 1x2048x32x128]
) {
  %/Constant_output_0 = Constant[value = <Tensor>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Trilu_output_0 = Trilu[upper = 1](%/Constant_output_0, %/Constant_1_output_0)
  %/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%q)
  %/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%v)
  %/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%k)
  %/MatMul_output_0 = MatMul(%/Transpose_output_0, %/Transpose_2_output_0)
  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Div_output_0 = Div(%/MatMul_output_0, %/Constant_2_output_0)
  %/Add_output_0 = Add(%/Div_output_0, %/Trilu_output_0)
  %/Cast_output_0 = Cast[to = 1](%/Add_output_0)
  %/Softmax_output_0 = Softmax[axis = -1](%/Cast_output_0)
  %/Cast_1_output_0 = Cast[to = 10](%/Softmax_output_0)
  %/MatMul_1_output_0 = MatMul(%/Cast_1_output_0, %/Transpose_1_output_0)
  %/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/MatMul_1_output_0)
  %onnx::ReduceSum_21 = Constant[value = <Tensor>]()
  %/ReduceSum_output_0 = ReduceSum[keepdims = 0](%/Softmax_output_0, %onnx::ReduceSum_21)
  %/Constant_3_output_0 = Constant[value = <Tensor>]()
  %out = Reshape[allowzero = 0](%/Transpose_3_output_0, %/Constant_3_output_0)
  %/Constant_4_output_0 = Constant[value = <Tensor>]()
  %h2o_score = Reshape[allowzero = 0](%/ReduceSum_output_0, %/Constant_4_output_0)
  return %out, %h2o_score
}
/home/ppopp25_ae/ppopp25_ae/3rd/asuka/python/asuka/translate.py:217: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  operand_torch = torch.from_numpy(operand)
module {
  func.func @H2O(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x2048x32x128xf16>) -> (tensor<1x2048x32x128xf16>, tensor<1x32x2048xf32>) {
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.softmax %7, dim = -1 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.convert %8, type = f16 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf16>
    %10 = asuka.dot %9, %2 : (tensor<1x32x2048x2048xf16>, tensor<1x32x2048x128xf16>) -> tensor<1x32x2048x128xf16>
    %11 = asuka.permute %10, dims = [0, 2, 1, 3] : (tensor<1x32x2048x128xf16>) -> tensor<1x2048x32x128xf16>
    %12 = asuka.reduce(%8), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048xf32>
    %13 = asuka.reshape %11 : (tensor<1x2048x32x128xf16>) -> tensor<1x2048x32x128xf16>
    %14 = asuka.reshape %12 : (tensor<1x32x2048xf32>) -> tensor<1x32x2048xf32>
    return %13, %14 : tensor<1x2048x32x128xf16>, tensor<1x32x2048xf32>
  }
}
idx=0 arith.constant
idx=1 asuka.trilu
idx=2 asuka.permute
idx=3 asuka.permute
idx=4 asuka.permute
idx=5 asuka.dot
idx=6 asuka.div
idx=7 asuka.add
idx=8 asuka.convert
idx=9 asuka.exp
idx=10 asuka.reduce
idx=11 asuka.div
idx=12 asuka.convert
idx=13 asuka.dot
idx=14 asuka.permute
idx=15 asuka.reduce
len(connected_partitions)=724
pruning bad partition(provide_output=False):   0%|          | 0/724 [00:00<?, ?it/s]pruning bad partition(provide_output=False):  31%|███       | 223/724 [00:00<00:00, 2223.60it/s]pruning bad partition(provide_output=False):  62%|██████▏   | 446/724 [00:00<00:00, 2103.37it/s]pruning bad partition(provide_output=False):  93%|█████████▎| 672/724 [00:00<00:00, 2170.86it/s]pruning bad partition(provide_output=False): 100%|██████████| 724/724 [00:00<00:00, 2178.94it/s]
after pruning bad partition: len(connected_partitions)=101
pruning bad para partition:   0%|          | 0/101 [00:00<?, ?it/s]pruning bad para partition:  64%|██████▍   | 65/101 [00:00<00:00, 639.19it/s]pruning bad para partition: 100%|██████████| 101/101 [00:00<00:00, 648.17it/s]
after pruning bad para partition: len(connected_partitions)=55
max_metric:  134217728.0
tall_and_thin tensors: 7
expand and pruning bad ai partition:   0%|          | 0/55 [00:00<?, ?it/s]expand and pruning bad ai partition:  56%|█████▋    | 31/55 [00:00<00:00, 55.61it/s]expand and pruning bad ai partition:  95%|█████████▍| 52/55 [00:00<00:00, 59.76it/s]expand and pruning bad ai partition: 100%|██████████| 55/55 [00:00<00:00, 61.36it/s]
after expand and pruning bad ai partition: len(connected_partitions)=10
pruning bad partition(provide_output=True):   0%|          | 0/10 [00:00<?, ?it/s]pruning bad partition(provide_output=True): 100%|██████████| 10/10 [00:00<00:00, 2263.15it/s]
after pruning bad partition(provide_output=True): len(connected_partitions)=5
module {
  func.func @H2O(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x2048x32x128xf16>) -> (tensor<1x2048x32x128xf16>, tensor<1x32x2048xf32>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg2, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %3 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.exp %7 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x2048x2048xf32>, tensor<1x32x2048x1xf32>) -> tensor<1x32x2048x2048xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x2048x2048xf16>, tensor<1x32x2048x128xf16>) -> tensor<1x32x2048x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x2048x128xf16>) -> tensor<1x2048x32x128xf16>
    %14 = asuka.reduce(%10), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048xf32>
    return %13, %14 : tensor<1x2048x32x128xf16>, tensor<1x32x2048xf32>
  }
  asuka.kernel @H2O_p0(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x2048x32x128xf16>, %arg3: tensor<1x32x2048x1xf32>) -> tensor<1x2048x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.exp %7 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.div %8, %arg3 : (tensor<1x32x2048x2048xf32>, tensor<1x32x2048x1xf32>) -> tensor<1x32x2048x2048xf32>
    %10 = asuka.convert %9, type = f16 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf16>
    %11 = asuka.dot %10, %2 : (tensor<1x32x2048x2048xf16>, tensor<1x32x2048x128xf16>) -> tensor<1x32x2048x128xf16>
    %12 = asuka.permute %11, dims = [0, 2, 1, 3] : (tensor<1x32x2048x128xf16>) -> tensor<1x2048x32x128xf16>
    asuka.return %12 : tensor<1x2048x32x128xf16>
  }
  asuka.kernel @H2O_p1(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x2048x32x128xf16>) -> (tensor<1x32x2048x1xf32>, tensor<1x2048x32x128xf16>) {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.exp %7 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x2048x2048xf32>, tensor<1x32x2048x1xf32>) -> tensor<1x32x2048x2048xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x2048x2048xf16>, tensor<1x32x2048x128xf16>) -> tensor<1x32x2048x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x2048x128xf16>) -> tensor<1x2048x32x128xf16>
    asuka.return %9, %13 : tensor<1x32x2048x1xf32>, tensor<1x2048x32x128xf16>
  }
  asuka.kernel @H2O_p2(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x2048x32x128xf16>) -> tensor<1x2048x32x128xf16> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %3 = asuka.permute %arg2, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %4 = asuka.dot %1, %3 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.div %4, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.add %5, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %7 = asuka.convert %6, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.exp %7 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.reduce(%8), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x1xf32>
    %10 = asuka.div %8, %9 : (tensor<1x32x2048x2048xf32>, tensor<1x32x2048x1xf32>) -> tensor<1x32x2048x2048xf32>
    %11 = asuka.convert %10, type = f16 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf16>
    %12 = asuka.dot %11, %2 : (tensor<1x32x2048x2048xf16>, tensor<1x32x2048x128xf16>) -> tensor<1x32x2048x128xf16>
    %13 = asuka.permute %12, dims = [0, 2, 1, 3] : (tensor<1x32x2048x128xf16>) -> tensor<1x2048x32x128xf16>
    asuka.return %13 : tensor<1x2048x32x128xf16>
  }
  asuka.kernel @H2O_p3(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x1xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %7 = asuka.exp %6 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.reduce(%7), dim = -1, op =  ADD, keep_dim = true : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x1xf32>
    asuka.return %8 : tensor<1x32x2048x1xf32>
  }
  asuka.kernel @H2O_p4(%arg0: tensor<1x2048x32x128xf16>, %arg1: tensor<1x2048x32x128xf16>, %arg2: tensor<1x32x2048x1xf32>) -> tensor<1x32x2048xf32> {
    %cst = arith.constant dense<1.131250e+01> : tensor<1xf16>
    %0 = asuka.trilu diagonal = 1, is_upper = true, shape = [2048, 2048], val = 0xFC00 : f16
    %1 = asuka.permute %arg0, dims = [0, 2, 1, 3] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x2048x128xf16>
    %2 = asuka.permute %arg1, dims = [0, 2, 3, 1] : (tensor<1x2048x32x128xf16>) -> tensor<1x32x128x2048xf16>
    %3 = asuka.dot %1, %2 : (tensor<1x32x2048x128xf16>, tensor<1x32x128x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %4 = asuka.div %3, %cst : (tensor<1x32x2048x2048xf16>, tensor<1xf16>) -> tensor<1x32x2048x2048xf16>
    %5 = asuka.add %4, %0 : (tensor<1x32x2048x2048xf16>, tensor<2048x2048xf16>) -> tensor<1x32x2048x2048xf16>
    %6 = asuka.convert %5, type = f32 : (tensor<1x32x2048x2048xf16>) -> tensor<1x32x2048x2048xf32>
    %7 = asuka.exp %6 : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %8 = asuka.div %7, %arg2 : (tensor<1x32x2048x2048xf32>, tensor<1x32x2048x1xf32>) -> tensor<1x32x2048x2048xf32>
    %9 = asuka.reduce(%8), dim = 2, op =  ADD, keep_dim = false : (tensor<1x32x2048x2048xf32>) -> tensor<1x32x2048xf32>
    asuka.return %9 : tensor<1x32x2048xf32>
  }
}
optimize H2O_p0
optimize H2O_p1
optimize H2O_p2
optimize H2O_p3
optimize H2O_p4
tuning time: 2.3910984992980957 sec
Skip op: func.func
path: /tmp/tmpf_u94d20.py
profiling...
out_str='[H2O_p0] avg_ms: 0.17642435431480408\n[H2O_p1] avg_ms: 0.17691129446029663\n[H2O_p2] avg_ms: 0.17739620804786682\n[H2O_p3] avg_ms: 0.18167628347873688\n[H2O_p4] avg_ms: 0.21162405610084534\n'
[H2O_p0] avg_ms: 0.17642435431480408
[H2O_p1] avg_ms: 0.17691129446029663
[H2O_p2] avg_ms: 0.17739620804786682
[H2O_p3] avg_ms: 0.18167628347873688
[H2O_p4] avg_ms: 0.21162405610084534
best_kernel:

H2O_p1
H2O_p4
best_time=0.38853535056114197
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from typing import Callable, Any, Optional, Tuple

def bench_H2O_p1():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  avg_ms = triton.testing.do_bench(lambda: H2O_p1(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[H2O_p1] avg_ms:', avg_ms)

def H2O_p1(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 2048, 1, dtype=torch.float32, device=dev)
  empty_ptr_4 = torch.empty(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  grid = (1, 16, 32)
  H2O_p1_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, empty_ptr_4, autotune_key)
  tensor_5 = empty_ptr_3
  tensor_6 = empty_ptr_4
  return tensor_5, tensor_6

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def H2O_p1_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  arg_4,
  autotune_key,
):
  pid_5 = tl.program_id(0)
  pid_6 = tl.program_id(1)
  pid_7 = tl.program_id(2)
  const_8 = 1.275311e-01
  const_9 = float('-inf')
  const_10 = 0.000000e+00
  const_11 = 0
  const_12 = 2048
  const_13 = 1
  const_14 = 4096
  const_15 = 128
  mul_16 = pid_6 * const_15
  mul_17 = mul_16 * const_14
  mul_18 = pid_7 * const_15
  add_19 = mul_17 + mul_18
  block_ptr_20 = tl.make_block_ptr(
    base=arg_0 + add_19,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_load_21 = tl.load(block_ptr_20)
  mul_22 = pid_7 * const_12
  add_23 = mul_16 + mul_22
  block_ptr_24 = tl.make_block_ptr(
    base=arg_3 + add_23,
    shape=(128, 1,),
    strides=(1, 1,),
    offsets=(0, 0,),
    block_shape=(128, 1,),
    order=(1, 0,),
  )
  block_ptr_25 = tl.make_block_ptr(
    base=arg_4 + add_19,
    shape=(128, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  converted_26 = const_8
  mul_27 = block_load_21 * converted_26
  mul_27 = mul_27.to(tl.float16)
  zero_28 = tl.zeros([128, 128], dtype=tl.float32)
  zero_29 = tl.zeros([128, 1], dtype=tl.float32)
  add_30 = mul_16 + const_15
  block_ptr_31 = tl.make_block_ptr(
    base=arg_2 + mul_18,
    shape=(128, 2048,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_ptr_32 = tl.make_block_ptr(
    base=arg_1 + mul_18,
    shape=(2048, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  for i_33 in range(const_11, add_30, const_15):
    block_load_34 = tl.load(block_ptr_31)
    block_load_35 = tl.load(block_ptr_32)
    dot_36 = tl.dot(mul_27, block_load_34)
    where_37 = tl.zeros([128, 128], dtype=tl.float32)
    where_37 = tl.where(mul_16 + tl.arange(0, 128)[:, None] >= i_33 + tl.arange(0, 128)[None, :], where_37, float('-inf'))
    add_38 = dot_36 + where_37
    exp2_39 = tl.math.exp2(add_38)
    reduce_sum_40 = tl.sum(exp2_39, axis=1, keep_dims=True).to(tl.float32)
    reduce_sum_40 += zero_29
    converted_41 = exp2_39.to(tl.float16)
    dot_42 = tl.dot(converted_41, block_load_35)
    add_43 = zero_28 + dot_42
    block_advance_44 = tl.advance(block_ptr_31, (0, 128,))
    block_advance_45 = tl.advance(block_ptr_32, (128, 0,))
    block_ptr_31 = block_advance_44
    block_ptr_32 = block_advance_45
    zero_28 = add_43
    zero_29 = reduce_sum_40
  div_46 = zero_28 / zero_29
  converted_47 = div_46.to(tl.float16)
  block_store_48 = tl.store(block_ptr_24, zero_29)
  block_store_49 = tl.store(block_ptr_25, converted_47)

def bench_H2O_p4():
  dev = torch.cuda.current_device()
  rand_arg_0 = torch.randn(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_1 = torch.randn(1, 2048, 32, 128, dtype=torch.float16, device=dev)
  rand_arg_2 = torch.randn(1, 32, 2048, 1, dtype=torch.float32, device=dev)
  avg_ms = triton.testing.do_bench(lambda: H2O_p4(rand_arg_0, rand_arg_1, rand_arg_2))
  print('[H2O_p4] avg_ms:', avg_ms)

def H2O_p4(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> torch.Tensor:
  dev = arg0.device
  autotune_key = torch.cuda.get_device_capability(dev)[0]
  tensor_0 = arg0
  tensor_1 = arg1
  tensor_2 = arg2
  empty_ptr_3 = torch.empty(1, 32, 2048, dtype=torch.float32, device=dev)
  grid = (1, 16, 32)
  H2O_p4_kernel[grid](tensor_0, tensor_1, tensor_2, empty_ptr_3, autotune_key)
  tensor_4 = empty_ptr_3
  return tensor_4

@triton.autotune(configs=[
  triton.Config({}, num_warps=4),
  triton.Config({}, num_warps=8),
], key=['autotune_key'])
@triton.jit
def H2O_p4_kernel(
  arg_0,
  arg_1,
  arg_2,
  arg_3,
  autotune_key,
):
  pid_4 = tl.program_id(0)
  pid_5 = tl.program_id(1)
  pid_6 = tl.program_id(2)
  const_7 = 1.275311e-01
  const_8 = float('-inf')
  const_9 = 0.000000e+00
  const_10 = 2048
  const_11 = 4096
  const_12 = 128
  const_13 = 1
  mul_14 = pid_6 * const_12
  mul_15 = pid_5 * const_12
  mul_16 = mul_15 * const_11
  add_17 = mul_16 + mul_14
  block_ptr_18 = tl.make_block_ptr(
    base=arg_1 + add_17,
    shape=(128, 128,),
    strides=(1, 4096,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(0, 1,),
  )
  block_load_19 = tl.load(block_ptr_18)
  mul_20 = pid_6 * const_10
  add_21 = mul_15 + mul_20
  block_ptr_22 = tl.make_block_ptr(
    base=arg_3 + add_21,
    shape=(128,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  converted_23 = const_7
  mul_24 = block_load_19 * converted_23
  mul_24 = mul_24.to(tl.float16)
  zero_25 = tl.zeros([128], dtype=tl.float32)
  block_ptr_26 = tl.make_block_ptr(
    base=arg_0 + add_17,
    shape=(2048, 128,),
    strides=(4096, 1,),
    offsets=(0, 0,),
    block_shape=(128, 128,),
    order=(1, 0,),
  )
  block_ptr_27 = tl.make_block_ptr(
    base=arg_2 + add_21,
    shape=(2048,),
    strides=(1,),
    offsets=(0,),
    block_shape=(128,),
    order=(0,),
  )
  for i_28 in range(mul_15, const_10, const_12):
    block_load_29 = tl.load(block_ptr_26)
    block_load_30 = tl.load(block_ptr_27)
    where_31 = tl.zeros([128, 128], dtype=tl.float32)
    where_31 = tl.where(i_28 + tl.arange(0, 128)[:, None] >= mul_15 + tl.arange(0, 128)[None, :], where_31, float('-inf'))
    dot_32 = tl.dot(block_load_29, mul_24)
    add_33 = dot_32 + where_31
    exp2_34 = tl.math.exp2(add_33)
    unsqueeze_35 = block_load_30[:, None]
    div_36 = exp2_34 / unsqueeze_35
    reduce_sum_37 = tl.sum(div_36, axis=0, keep_dims=False).to(tl.float32)
    reduce_sum_37 += zero_25
    block_advance_38 = tl.advance(block_ptr_26, (128, 0,))
    block_advance_39 = tl.advance(block_ptr_27, (128,))
    block_ptr_26 = block_advance_38
    block_ptr_27 = block_advance_39
    zero_25 = reduce_sum_37
  block_store_40 = tl.store(block_ptr_22, zero_25)

def H2O(arg_0, arg_1, arg_2):
  k0_out_0, k0_out_1 = H2O_p1(arg_0, arg_2, arg_1)
  k1_out_0 = H2O_p4(arg_0, arg_1, k0_out_0)
  return k0_out_1, k1_out_0

write code to /tmp/tmpupryeqxq.py
start_peak_mem_mib=80.0
end_peak_mem_mib=352.5
gflops=34.359738368
mib=352.5
checking our...
out loss: {'abs_max': 0.0029296875, 'abs_mean': 3.694971599799146e-05, 'rel_max': inf, 'rel_mean': inf, 'nz_rel_max': 1458.0, 'nz_rel_mean': 0.006207545209278474, 'mse': 3.9687033922748895e-09, 'rmse': 6.299764592645419e-05}
h2o_score loss: {'abs_max': 0.0012454986572265625, 'abs_mean': 3.884111761232045e-05, 'rel_max': 0.0011807274313306991, 'rel_mean': 4.668393411858954e-05, 'nz_rel_max': 0.0011807274313306991, 'nz_rel_mean': 4.668393411858954e-05, 'mse': 5.006897398906772e-09, 'rmse': 7.075943328565295e-05}
warmup start
warmup done
[our] avg 0.4530 ms, min 0.4435 ms, max 0.5045 ms, 75850.09898729257 gflops/s (10 runs, 100 warmups, profiled)
